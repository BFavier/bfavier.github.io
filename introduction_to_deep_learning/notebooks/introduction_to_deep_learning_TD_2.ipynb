{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benoit\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Iterable\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Déplacer un Tensor sur GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Déplacer un Module sur GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Les convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice I\n",
    "\n",
    "Définir et entraîner un modèle de classification d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"../datasets/CIFAR10/CIFAR-10.npy\")\n",
    "y = np.load(\"../datasets/CIFAR10/CIFAR-10-labels.npy\")\n",
    "labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(*args: tuple[np.ndarray], fracs: tuple[float] = (0.7, 0.15, 0.15)) -> list[tuple[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    split data in \n",
    "    \"\"\"\n",
    "    total = sum(fracs)\n",
    "    fracs = tuple(f/total for f in fracs)\n",
    "    L = len(args[0])\n",
    "    indexes = np.random.permutation(L)\n",
    "    start = 0\n",
    "    results = []\n",
    "    for f in fracs:\n",
    "        end = start + int(f*L)\n",
    "        results.append(tuple(array[indexes[start:end]] for array in args))\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = test_data\n",
    "f, axes = plt.subplots(figsize=[10, 10], nrows=3, ncols=3)\n",
    "i = 0\n",
    "for axs in axes:\n",
    "    for ax in axs:\n",
    "        ax.imshow(x_test[i].transpose(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(labels[y_test[i]])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A block is a set of convolution/max pooling/activation/batch normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, activation: Callable = torch.relu, kernel_size: tuple[int] = (3, 3), pool: tuple[int, int] = (2, 2)):\n",
    "        super().__init__()\n",
    "        self.convolution = torch.nn.Conv2d(in_features, out_features, kernel_size, (1, 1), padding=\"same\")\n",
    "        self.pool = torch.nn.MaxPool2d(pool)\n",
    "        self.activation = activation\n",
    "        self.batch_norm = torch.nn.BatchNorm2d(out_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.batch_norm(self.activation(self.pool(self.convolution(X))))\n",
    "\n",
    "\n",
    "class ImageClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    An Image classifier is a CNN that classifies Images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, n_classes: int, features: list[int], activation: Callable = torch.relu, kernel_size: tuple[int] = (3, 3), pool: tuple[int, int] = 2):\n",
    "        super().__init__()\n",
    "        self.blocks = torch.nn.ModuleList()\n",
    "        for out_features in features:\n",
    "            self.blocks.append(Block(in_features, out_features, activation, kernel_size, pool))\n",
    "            in_features = out_features\n",
    "        self.output = torch.nn.Conv2d(out_features, n_classes, kernel_size=(1, 1), padding=\"same\")\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            tensor of floats of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        X = X.to(self.device)\n",
    "        for block in self.blocks:\n",
    "            X = block(X)\n",
    "        X = self.output(X)\n",
    "        N, C, H, W = X.shape\n",
    "        return X.reshape(N, C, -1).mean(dim=-1)\n",
    "\n",
    "    def predict(self, X: torch.Tensor):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Y = self(X)\n",
    "        return Y.max(dim=1).indices\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    returns the accuracy metric of a prediction\n",
    "    \"\"\"\n",
    "    assert predicted.shape == target.shape\n",
    "    return torch.sum(predicted == target).detach().cpu().item()/len(target)\n",
    "\n",
    "def inputs_to_tensor(x, device: torch.device = \"cuda:0\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    converts an image array input into tensor\n",
    "    \"\"\"\n",
    "    return torch.tensor(x/255., dtype=torch.float32, device=device)\n",
    "\n",
    "def target_to_tensors(y: np.ndarray, device: torch.device = \"cuda:0\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    converts a target numpy array to tensor\n",
    "    \"\"\"\n",
    "    return torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "def arrays_to_tensors(x: np.ndarray, y: np.ndarray, device: torch.device = \"cuda:0\") -> tuple[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    transforms numpy arrays to tensors\n",
    "    \"\"\"\n",
    "    return (inputs_to_tensor(x, device), target_to_tensors(y, device))\n",
    "\n",
    "def batchifyer(*args: tuple[np.ndarray], device: torch.device = \"cuda:0\", n_batches: int = 1, batch_size: int = 100) -> Iterable[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    transforms numpy array into batches of torch tensor\n",
    "    \"\"\"\n",
    "    L = len(args[0])\n",
    "    indexes = np.random.permutation(L)\n",
    "    for i in range(n_batches):\n",
    "        x, y = (array[indexes[i*batch_size:(i+1)*batch_size]] for array in args)\n",
    "        yield arrays_to_tensors(x, y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_data: tuple[torch.Tensor], val_data: tuple[torch.Tensor], n_steps: int = 1000, n_batches: int = 1, batch_size: int = 100, patience: int = 100, keep_best: bool = True):\n",
    "    \"\"\"\n",
    "    train the model for the specified number of steps, or untilearly stopping\n",
    "    \"\"\"\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_step = 0\n",
    "    best_metric = 0.\n",
    "    try:\n",
    "        for step in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "            # train loss\n",
    "            model.train()\n",
    "            losses = []\n",
    "            for batch in batchifyer(*train_data, device=model.device, n_batches=n_batches, batch_size=batch_size):\n",
    "                x, y = batch\n",
    "                y_pred = model(x)\n",
    "                loss = F.cross_entropy(y_pred, y)\n",
    "                loss.backward()\n",
    "                losses.append(loss.item())\n",
    "            loss = sum(losses)/len(losses)\n",
    "            # val metric\n",
    "            model.eval()\n",
    "            metrics = []\n",
    "            for batch in batchifyer(*val_data, device=model.device, n_batches=n_batches, batch_size=batch_size):\n",
    "                x, y = batch\n",
    "                y_pred = model.predict(x)\n",
    "                metrics.append(accuracy(y_pred, y))\n",
    "            metric = sum(metrics) / len(metrics)\n",
    "            # checkpointing\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_step = step\n",
    "                if keep_best:\n",
    "                    best_state = deepcopy(model.state_dict())\n",
    "            elif step - best_step > patience:\n",
    "                print(\"early stoping\")\n",
    "                break\n",
    "            # optimizer steping\n",
    "            optimizer.step()\n",
    "            # printing\n",
    "            print(f\"Step {step}: loss = {loss:.3g} accuracy = {metric:.2%}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted by user\")\n",
    "    if keep_best:\n",
    "        model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassifier(3, len(labels), [32, 64, 128, 256])\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0E-3)\n",
    "train_loop(model, optimizer, train_data, val_data, n_steps=10000, n_batches=1, batch_size=10000, patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = arrays_to_tensors(*test_data)\n",
    "y_pred = model.predict(x)\n",
    "print(accuracy(y_pred, torch.tensor(y, device=y_pred.device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = test_data\n",
    "f, axes = plt.subplots(figsize=[10, 10], nrows=3, ncols=3)\n",
    "i = 0\n",
    "for axs in axes:\n",
    "    for ax in axs:\n",
    "        ax.imshow(x_test[i].transpose(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(labels[y_pred[i]], color=\"g\" if y_pred[i] == y_test[i] else \"r\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice II\n",
    "\n",
    "Définir et entraîner un modèle de segmentation d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polygons(image_shape: tuple[int, int], delta: int = 10, n_max_vertices: int = 5):\n",
    "    \"\"\"\n",
    "    generate a set of random polygon parametrized as a list of (x, y) tuples\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_shape : tuple of int\n",
    "        the (height, width) in pixels of the image to fill with polygons\n",
    "    delta : int\n",
    "        the \n",
    "    \"\"\"\n",
    "    assert n_max_vertices >= 3\n",
    "    x_offset, y_offset = np.random.uniform(-delta, delta, 2)  # offset of the polygons grid\n",
    "    x_spacing, y_spacing = np.arange(-delta, image_shape[1]+delta, delta)+x_offset, np.arange(-delta, image_shape[0]+delta, delta)+y_offset\n",
    "    x_centers, y_centers = np.meshgrid(x_spacing, y_spacing, indexing=\"xy\")  # coordinates of polygons centers\n",
    "    Lx, Ly = x_centers.shape\n",
    "    x_centers += np.random.uniform(-delta/2, delta/2, (Lx, Ly))\n",
    "    y_centers += np.random.uniform(-delta/2, delta/2, (Lx, Ly))\n",
    "    thetas = np.cumsum(np.random.uniform(1., 3., (Lx, Ly, n_max_vertices+1)), axis=-1)  # angle of each vertex of each polygon\n",
    "    n_vertices = np.random.randint(3, n_max_vertices+1, (Lx, Ly, 1))  # number of vertices in each polygon\n",
    "    thetas = thetas[..., :-1] * 2*np.pi / np.take_along_axis(thetas, n_vertices, axis=-1)\n",
    "    mu, sigma = delta/3, delta/6\n",
    "    radius = np.clip(np.random.normal(mu, sigma, (Lx, Ly, n_max_vertices)), 0, delta/2)  # radius of each vertex of eahc polygon\n",
    "    return [[(x_center + r*np.cos(theta), y_center + r*np.sin(theta)) for r, theta, _ in zip(points_r, points_theta, range(point_n_vertices))]\n",
    "            for points_r, points_theta, x_center, y_center, point_n_vertices\n",
    "            in zip(radius.reshape(-1, n_max_vertices), thetas.reshape(-1, n_max_vertices), x_centers.reshape(-1), y_centers.reshape(-1), n_vertices.reshape(-1))]\n",
    "\n",
    "def generate_image(width: int = 64, height: int = 64, delta: int = 10):\n",
    "    \"\"\"\n",
    "    generate a an image of a polygon and it's target interior\n",
    "    \"\"\"\n",
    "    shape = (height, width)\n",
    "    img = Image.new(\"L\", shape)\n",
    "    target = Image.new(\"1\", shape)\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "    target_draw = ImageDraw.Draw(target)\n",
    "    n_polygons = np.random.randint(3, 6)\n",
    "    for polygon in generate_polygons(shape, delta):\n",
    "        img_draw.polygon(polygon, fill=None, outline=\"#ffffff\")\n",
    "        target_draw.polygon(polygon, fill=\"#ffffff\", outline=\"#ffffff\")\n",
    "    return np.array(img, dtype=np.uint8), np.array(target, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, 5) (7, 7, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = generate_image(64, 64, 15)\n",
    "f, axes = plt.subplots(figsize=[10, 5], ncols=2)\n",
    "for ax, img in zip(axes, [x, y]):\n",
    "    ax.imshow(img, cmap=\"viridis\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2017a09ab1f54c7a6a1af190715fd60264df4a93389a277e03c18d947a6e489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

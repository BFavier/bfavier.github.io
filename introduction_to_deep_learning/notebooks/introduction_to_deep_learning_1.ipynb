{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Les tenseurs\n",
    "\n",
    "L'outil central de la librairie et la classe torch.Tensor qui permet de faire des calculs vectorizés et être indéxé à la manière d'une array numpy.\n",
    "C'est un bloc contigue de mémoire qui contient des données typées, et est découpé en plusieurs dimensions. En plus du type de données, on peut définir si le tenseur requiert la propagation du gradient, et l'appareil sur lequel les données sont stockées et où les opérations seront effectuées (\"cpu\" ou \"cuda:i\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 5], [3, 2]], dtype=torch.float32, requires_grad=False, device=\"cpu\")  # la fonction \"torch.tensor\" permet de créer un tenseur de manière analogue à \"np.array\"\n",
    "print(repr(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*t - t**0.5 + 2  # Les opérations usuelles en numpy sont disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t.shape, t.dtype, t.requires_grad, t.device)  # les attributs usuels des array numpy sont nommées de manière identiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[1, 1], t[:, 1], t[..., 0], t[0], t[:, :1]  # Les syntaxes d'indentation sont identiques à celles de numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les opérations usuelles de la librairie numpy existent dans pytorch (ou presque). Si le tenseur est sur un GPU, le calcul sera effectué de manière parallélisée sur GPU, ce qui peut très largement accélérer la vitesse d'execution pour des tenseurs de grande taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.min(), t.mean(dim=1), t.max(), t.std(dim=0)  # les méthodes usuelles des array numpy sont nommées de manière identiques. Le kwargs \"ax\" est renommé \"dim\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t @ -t  # Les opérateur aux sens spécifiques en numpy sont aussi implémentés. Ici équivalent à \"torch.matmul(t, -t)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(-t**2 + 2)  # les fonctions mathématiques courantes sont disponibles dans le namespace \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.elu(-t)  # Les fonctions liées à des opérations propres aux réseaux de neurones (loss, convolutions, activation, ...) se trouvent dans le namespace \"torch.nn.functional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.inv(t)  # Les fonctions d'algèbre linéaires de numpy sont implémentées dans le namespace \"torch.linalg\". Ici une opération d'inversion matricielle (dont on peut obtenir le gradient !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand((1,)), torch.randint(0, 3, (1,)), torch.normal(0., 1., (1,))  # Les fonctions de génération de nombres aléatoires ne sont pas (contrairement à numpy) dans le namespace torch.random !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout comme les arrays numpy, on peut réorganiser la taille des dimensions (sans que la taille totale du tenseur ne change) ou l'ordre des dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(2*3*3, dtype=torch.long).reshape(3, 2, 3)\n",
    "print(repr(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.transpose(1, 2)  # inverse l'ordre des dimensions 1 et 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.permute(1, 0, 2)  # permute l'ordre de toutes les dimensions dans l'ordre spécifié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.unsqueeze(0).shape  # la méthode \"unsqueeze\" permet de créer des dimensions vide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.unsqueeze(0).squeeze(0).shape  # la méthode \"squeeze\" à l'inverse permet de retirer une dimension vide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) création du graph de calcul\n",
    "\n",
    "Lorsqu'un tenseur requiert la propagation du gradient, le graphe de calcul est créé dynamiquement, de manière transparente pour l'utilisateur, lorsqu'on effectue des opérations, le résultat renvoyé est un nouveau tenseur lié au précédent par une branche du graphe deu calcul. Pour cette raison, il ne faut jamais modifier manuellement le contenu des tenseurs à travers lesquels on compte propager le gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(4, dtype=torch.float32, requires_grad=True)\n",
    "print(repr(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.sum(t**2)  # notez la présence de la \"grad_fn\" (gradient function) qui a donné lieu au nouveau tenseur.\n",
    "print(repr(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(r, t, retain_graph=True)  # la fonction \"grad\" permet de calculer le gradient d'un scalaire par apport à un tenseur donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.backward()  # la méthode \"backward\" permet de propager le gradient dans tous le graph\n",
    "print(repr(t.grad))  # on retrouve la solution analytique de la dérivée de la somme des carrés (y = 2*x), évaluées en [0, 1, 2, 3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une fois le gradient propagé, les résultats intermédiaires du graph de calcul sont supprimés pour libérer de la mémoire.\n",
    "# On ne peut pas appeler backward une deuxième fois\n",
    "try:\n",
    "    r.backward()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(4, dtype=torch.float32, requires_grad=True)\n",
    "r = torch.sum(t**2)  # appeler \"backward\" avec le flag \"retain_graph\" à \"True\" permet de ne pas effacer le graphe de calcule.\n",
    "r.backward(retain_graph=True)\n",
    "r.backward()\n",
    "print(repr(t.grad))  # On remarque alors que pytorch \"accumule\" les gradients au lieux d'écrire la nouvelle valuer par dessus l'acienne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser un contexte \"torch.no_grad()\" permet de ne pas construire le graph de calcul, et donc d'économiser la mémoire, et d'accélérer le calcul\n",
    "t = torch.arange(4, dtype=torch.float32, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    r = torch.sum(t**2)\n",
    "print(r.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Les Modules pytorch\n",
    "\n",
    "Dans pytorch une brique de modèle (une couche d'un modèle feed forward, ...) comme un modèle entier dérivent de la classe Module.\n",
    "Les classes filles doivent implémenter la fonction \"forward\" qui correspond à la passe avant de la couche/du modèle (aka: l'opération effectuée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = torch.nn.Linear(2, 3)  # certaines couches courrament utilisées sont déjà implémentées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lin)  # La couche linéaire (y = x@A.T + b) attend en entrée un tenseur de shape (n_observations, features_in) et renvoit un tenseur de shape (n_observations, features_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = lin(torch.rand(10, 2))  # on peut appliquer l'opération correspondante en utilisant l'opérateur __call__\n",
    "print(repr(r.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lin.parameters())  # on peut obtenir un itérable des paramètres d'un module ave cla méthode \"parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class Layer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Un objet 'Layer' est une couche cachée d'un réseau feed forward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, activation: Callable, dropout: float):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            number of features in input tensors\n",
    "        out_features : int\n",
    "            number of features in output\n",
    "        activation : Callable\n",
    "            activation function applied after linear projection and batch normalization\n",
    "        dropout : float\n",
    "            dropout probability for gidden layers\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor :\n",
    "            tensor of shape (N, H_out)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(out_features)\n",
    "        self.activation = activation\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            tensor of shape (N, H_in)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor :\n",
    "            tensor of shape (N, H_out)\n",
    "        \"\"\"\n",
    "        X = self.linear(X)\n",
    "        X = self.batch_norm(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.dropout(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardRegressor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Un objet 'FeedForwardRegressor' est un modèle feed forward pour la régression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_features: list[int], activation: Callable = F.relu, dropout: float = 0.):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        for out_features in hidden_features:\n",
    "            self.hidden_layers.append(Layer(in_features, out_features, activation, dropout))\n",
    "            in_features = out_features\n",
    "        self.output_projection = torch.nn.Linear(out_features, 1)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            tensor of shape (N, H_in)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor :\n",
    "            tensor of shape (N, H_out)\n",
    "        \"\"\"\n",
    "        for layer in self.hidden_layers:\n",
    "            X = layer(X)\n",
    "        return self.output_projection(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffr = FeedForwardRegressor(2, [10, 10, 10])\n",
    "ffr(torch.rand(10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ffr.children())  # On peut récupérer un itérable sur les sous modules avec la méthode \"children\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(ffr.parameters()))  # La méthode \"parameter\" renvoit aussi les groupes de paramètres des sous modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certains Modules ne doivent pas avoir le même comportement lors de l'entraînement et de l'évaluation.\n",
    "# Un flag permet de déterminer si le module est en mode entraînement ou évaluation\n",
    "dropout = torch.nn.Dropout(p=0.5)\n",
    "dropout.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut passer un module (et ses sous modules recursivement) en mode entraînement avec la méthode \"train\"\n",
    "dropout.train()\n",
    "dropout(torch.rand(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut passer un module (et ses sous modules recursivement) en mode entraînement avec la méthode \"eval\"\n",
    "dropout.eval()\n",
    "dropout(torch.rand(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) La boucle d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x**2 + torch.normal(0., 0.1, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.linspace(-1., 1., 160).unsqueeze(1)\n",
    "y_train = y(x_train)\n",
    "x_val = torch.rand((40, 1)) * 2 - 1\n",
    "y_val = y(x_val)\n",
    "plt.scatter(x_train.reshape(-1).tolist(), y_train.reshape(-1).tolist(), label=\"training\")\n",
    "plt.scatter(x_val.reshape(-1).tolist(), y_val.reshape(-1).tolist(), label=\"validation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred: torch.Tensor, y_target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    returns the mean squared error loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : torch.Tensor\n",
    "        tensor of shape (N,)\n",
    "    y_target : torch.Tensor\n",
    "        tensor of shape (N,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor :\n",
    "        scalar tensor\n",
    "    \"\"\"\n",
    "    return F.mse_loss(y_pred, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, optimier: torch.optim.Optimizer, train_data: tuple[torch.Tensor], val_data: tuple[torch.Tensor], loss_function: Callable, n_steps: int = 100, patience: int = 10):\n",
    "    \"\"\"\n",
    "    performs the training for a given number of iterations\n",
    "    \"\"\"\n",
    "    x_train, y_train = train_data\n",
    "    x_val, y_val = val_data\n",
    "    best_metric = float(\"inf\")\n",
    "    best_step = 0\n",
    "    checkpoint = deepcopy(model.state_dict())\n",
    "    for step in range(n_steps):\n",
    "        # setting the gradient to zero to avoid gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        # training\n",
    "        model.train()\n",
    "        loss = loss_function(model(x_train), y_train)\n",
    "        loss.backward()\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            metric = loss_function(model(x_val), y_val).item()\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_step = step\n",
    "            checkpoint = deepcopy(model.state_dict())\n",
    "        elif step - best_step == patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "        # printing progress\n",
    "        print(f\"Step {step}: loss = {loss.item():.3g}, metric = {metric:.3g}\", flush=True)\n",
    "        # step of the optimizer\n",
    "        optimizer.step()\n",
    "    model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le namespace \"optim\" contient les différents algorithmes d'optimisation\n",
    "ffr = FeedForwardRegressor(1, [30, 30])\n",
    "optimizer = torch.optim.Adam(ffr.parameters(), lr=1.0E-3)\n",
    "train_loop(ffr, optimizer, (x_train, y_train), (x_val, y_val), F.mse_loss, n_steps=200, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(-1, 1, 1000)\n",
    "y_target = x_test**2\n",
    "y_pred = ffr(x_test.unsqueeze(-1)).squeeze(-1)\n",
    "plt.plot(x_test.tolist(), y_target.tolist(), color=\"k\", label=\"target\")\n",
    "plt.plot(x_test.tolist(), y_pred.tolist(), color=\"r\", label=\"prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V) Une loss pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    returns the binary classification of the given (x1, x2) points\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        tensor of shape (n, 2) of floats in [-1, 1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        tensor of shape (n, 1) of type long, of values in {0, 1}\n",
    "    \"\"\"\n",
    "    L = 0.8\n",
    "    return (((x[:, 0] + x[:, 1]) % L) > L/2).float().unsqueeze(-1)\n",
    "\n",
    "\n",
    "def generate_data(n: int) -> tuple[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    generate an (x, y) tuple of tensors of n random observations\n",
    "    \"\"\"\n",
    "    x = torch.rand(n, 2) * 2 - 1\n",
    "    y_ = y(x)\n",
    "    return x, torch.where(torch.rand(n, 1) < 0.05, 1-y_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_data(800)\n",
    "x_val, y_val = generate_data(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = y(torch.stack(torch.meshgrid([torch.linspace(-1., 1., 500)]*2, indexing=\"ij\"), dim=-1).reshape(-1, 2)).reshape(500, 500)\n",
    "plt.imshow(image, extent=(-1, 1, -1, 1), origin=\"lower\", cmap=\"Greys\", vmin=0, vmax=1)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=[f\"C{c.item()}\" for c in y_train.int()], marker=\".\", label=\"train\")\n",
    "plt.scatter(x_val[:, 0], x_val[:, 1], c=[f\"C{c.item()}\" for c in y_val.int()], marker=\"+\", label=\"validation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Un objet 'FeedForwardRegressor' est un modèle feed forward pour la classification binaire\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_layers: list[int], activation: Callable = F.relu, dropout: float = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for out_features in hidden_layers:\n",
    "            self.layers.append(Layer(in_features, out_features, activation, dropout))\n",
    "            in_features = out_features\n",
    "        self.output_projection = torch.nn.Linear(out_features, 1)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return torch.sigmoid(self.output_projection(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffc = FeedForwardClassifier(2, [30, 30, 30, 30])\n",
    "optimizer = torch.optim.Adam(ffc.parameters(), lr=1.0E-3)\n",
    "train_loop(ffc, optimizer, (x_train, y_train), (x_val, y_val), F.binary_cross_entropy, n_steps=1000, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = ffc(torch.stack(torch.meshgrid([torch.linspace(-1., 1., 500)]*2, indexing=\"ij\"), dim=-1).reshape(-1, 2)).reshape(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.numpy(), origin=\"lower\", cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) Exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/parcoursup_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['code_UAI', 'nom_etablissement', 'code_departement', 'nom_formation',\n",
       "       'code_formation_parcoursup', 'type_contrat', 'formation_selective',\n",
       "       'concour', 'nombre_candidats', 'taux_admission', 'taux_femmes',\n",
       "       'taux_boursiers', 'taux_meme_academie', 'taux_meme_etablissement',\n",
       "       'taux_bac_technologique', 'taux_bac_pro', 'taux_mention_assez_bien',\n",
       "       'taux_mention_bien', 'taux_mention_tres_bien',\n",
       "       'taux_mention_tres_bien_felicitations'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df.sample(frac=0.15)\n",
    "df_other = df.drop(df_eval.index)\n",
    "df_train = df_other.sample(frac=0.8235)\n",
    "df_val = df_other.drop(df_train.index)\n",
    "df_test = df_other.sample(frac=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../datasets/train.csv\", index=False)\n",
    "df_test.to_csv(\"../datasets/test.csv\", index=False)\n",
    "df_eval.to_csv(\"../datasets/eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2017a09ab1f54c7a6a1af190715fd60264df4a93389a277e03c18d947a6e489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from typing import Callable, Iterable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialToken:\n",
    "    \"\"\"\n",
    "    A special token for tokenizers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, string: str):\n",
    "        self.string = string.upper()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.string}>\"\n",
    "    \n",
    "    def __eq__(self, other) -> bool:\n",
    "        if isinstance(other, SpecialToken):\n",
    "            return self.string == other.string\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.string)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    A simple word tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    word_pattern = re.compile(R\"\\w+|\\d+|[^\\w\\d\\s]\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tokenizer({len(self.vocabulary)} tokens)\"\n",
    "\n",
    "    def __init__(self, corpus: Iterable[str], min_frequency: float = 1.0E-6):\n",
    "        words = [word for document in corpus for word in self._split(document)]\n",
    "        word_counts = Counter(words)\n",
    "        word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.vocabulary = [k for k, v in word_counts if v/len(words) >= min_frequency] + [SpecialToken(\"UNKNOWN\"), SpecialToken(\"END\"), SpecialToken(\"PAD\")]\n",
    "        self.map = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def split(self, document: str) -> list[str]:\n",
    "        return self.decode(self.encode(document))\n",
    "\n",
    "    def encode(self, document: str) -> list[int]:\n",
    "        return [self.map.get(word, self.UNKNOWN) for word in self._split(document)]\n",
    "\n",
    "    def decode(self, encoded: list[int]) -> list[str]:\n",
    "        return [i.string if isinstance(i, SpecialToken) else self.vocabulary[i] for i in encoded]\n",
    "\n",
    "    def _split(self, document: str) -> list[str]:\n",
    "        return self.word_pattern.findall(document)\n",
    "\n",
    "    @property\n",
    "    def PAD(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"PAD\")]\n",
    "    \n",
    "    @property\n",
    "    def END(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"END\")]\n",
    "    \n",
    "    @property\n",
    "    def UNKNOWN(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"UNKNOWN\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/Twitter_US_Airline_Sentiment.csv\")\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(df.text, min_frequency=1.0E-5)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.sample(n=10).iterrows():\n",
    "    print(row.text)\n",
    "    print(row.airline_sentiment)\n",
    "    print(tokenizer.split(row.text))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7)\n",
    "df = df.drop(index=df_train.index)\n",
    "df_val = df.sample(frac=0.5)\n",
    "df_test = df.drop(index=df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted: torch.Tensor, target: torch.Tensor):\n",
    "    assert target.shape == predicted.shape\n",
    "    assert target.dtype == torch.long\n",
    "    assert predicted.dtype == torch.long\n",
    "    with torch.no_grad():\n",
    "        return torch.mean((predicted == target).float()).cpu().item()\n",
    "\n",
    "\n",
    "def input_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> torch.Tensor:\n",
    "    encoded = [tokenizer.encode(document) for document in df['text']]\n",
    "    L = max([len(doc) for doc in encoded])\n",
    "    encoded = [doc + [tokenizer.PAD]*(L - len(doc)) for doc in encoded]\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "def target_to_tensor(df: pd.DataFrame) -> torch.Tensor:\n",
    "    map = {k: v for v, k in enumerate(labels)}\n",
    "    return torch.tensor([map[label] for label in df[\"airline_sentiment\"]], dtype=torch.long)\n",
    "\n",
    "\n",
    "def data_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> tuple[torch.Tensor]:\n",
    "    return (input_to_tensor(df, tokenizer), target_to_tensor(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batchifyer:\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: Tokenizer, n_batches: int, batch_size: Optional[int]):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_batches = n_batches\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        shuffled = df.sample(frac=1.)\n",
    "        return (self._batch(shuffled, i) for i in range(self.n_batches))\n",
    "    \n",
    "    def _batch(self, shuffled: pd.DataFrame, i: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = self.batch_size or len(shuffled) // self.n_batches\n",
    "        subset = shuffled.iloc[i*batch_size:(i+1)*batch_size]\n",
    "        return data_to_tensor(subset, self.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_data: Iterable[tuple[torch.Tensor]], val_data: Iterable[tuple[torch.Tensor]], n_steps: int = 1000, patience: int = 100, keep_best: bool = True):\n",
    "    \"\"\"\n",
    "    train the model for the specified number of steps, or untilearly stopping\n",
    "    \"\"\"\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_step = 0\n",
    "    best_metric = 0.\n",
    "    try:\n",
    "        for step in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "            # train loss\n",
    "            model.train()\n",
    "            losses = []\n",
    "            for x, y in train_data:\n",
    "                loss = model.loss(x, y)\n",
    "                loss.backward()\n",
    "                losses.append(loss.item())\n",
    "            loss = sum(losses)/len(losses)\n",
    "            # val metric\n",
    "            model.eval()\n",
    "            metrics = []\n",
    "            for x, y in val_data:\n",
    "                metrics.append(model.metric(x, y))\n",
    "            metric = sum(metrics) / len(metrics)\n",
    "            # checkpointing\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_step = step\n",
    "                if keep_best:\n",
    "                    best_state = deepcopy(model.state_dict())\n",
    "            elif step - best_step > patience:\n",
    "                print(\"early stoping\")\n",
    "                break\n",
    "            # optimizer steping\n",
    "            optimizer.step()\n",
    "            # printing\n",
    "            print(f\"Step {step}: loss = {loss:.3g} metric = {metric:.2%}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted by user\")\n",
    "    if keep_best:\n",
    "        model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "\n",
    "Implémenter et entraîner un réseau récurrent pour classifier les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes: int, tokenizer: Tokenizer, in_features: int, hidden_state_features: int, activation: Callable = torch.relu):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_state_features = hidden_state_features\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.vocabulary), in_features)\n",
    "        self.linear = torch.nn.Linear(in_features + hidden_state_features, in_features + hidden_state_features)\n",
    "        self.activation = activation\n",
    "        self.contract = torch.nn.Linear(in_features + hidden_state_features, hidden_state_features)\n",
    "        self.normalization = torch.nn.LayerNorm(hidden_state_features)\n",
    "        self.output = torch.nn.Linear(hidden_state_features, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : torch.Tensor\n",
    "            tensor of long of shape (N, L)\n",
    "        \"\"\"\n",
    "        X = X.to(self.device)\n",
    "        N, L = X.shape\n",
    "        H = torch.zeros((N, self.hidden_state_features), dtype=torch.float32, device=X.device)\n",
    "        for x in X.transpose(0, 1):\n",
    "            I = self.embedding(x)\n",
    "            T = torch.cat([I, H], dim=1)\n",
    "            T = self.linear(T)\n",
    "            T = self.activation(T)\n",
    "            T = self.contract(T)\n",
    "            H = torch.where(x.unsqueeze(1) == self.tokenizer.PAD, H, T)\n",
    "        return self.output(H)\n",
    "    \n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Y = self(X)\n",
    "        return Y.max(dim=1).indices\n",
    "    \n",
    "    def loss(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self(X)\n",
    "        return F.cross_entropy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    def metric(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(len(labels), tokenizer, 100, 100)\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0E-4)\n",
    "train = Batchifyer(df_train, tokenizer, n_batches=1, batch_size=None)\n",
    "val = Batchifyer(df_val, tokenizer, n_batches=1, batch_size=None)\n",
    "train_loop(model, optimizer, train, val, n_steps=1000, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_to_tensor(df_test, tokenizer)\n",
    "y_pred = model.predict(X)\n",
    "acc = accuracy(y_pred, Y.to(y_pred.device))\n",
    "print(f\"accuracy {acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice II\n",
    "\n",
    "Programmer un modèle type encodeur de transformeur pour classifier les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, projection_dim: int, n_heads: int, activation: Callable):\n",
    "        super().__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.n_heads = n_heads\n",
    "        D = projection_dim * n_heads\n",
    "        self.q = torch.nn.Linear(D, D, bias=False)\n",
    "        self.k = torch.nn.Linear(D, D, bias=False)\n",
    "        self.v = torch.nn.Linear(D, D, bias=False)\n",
    "        self.intermediate_norm = torch.nn.LayerNorm(D)\n",
    "        self.expand = torch.nn.Linear(D, 4*D)\n",
    "        self.activation = activation\n",
    "        self.contract = torch.nn.Linear(4*D, D)\n",
    "        self.out_norm = torch.nn.LayerNorm(D)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor, mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            tensor of shape (N, L, D)\n",
    "        mask : torch.Tensor\n",
    "            tensor of boooleans of shape (N, L, L)\n",
    "        \"\"\"\n",
    "        input = X\n",
    "        N, L, D = X.shape\n",
    "        X = X.reshape(-1, D)\n",
    "        Q = self.q(X)\n",
    "        Q = Q.reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        K = self.k(X).reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        V = self.v(X).reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        S = torch.einsum(\"nhld, nhkd -> nhlk\", Q, K)\n",
    "        S = torch.masked_fill(S, mask.unsqueeze(1), -float(\"inf\"))\n",
    "        S = torch.softmax(S, dim=-1)\n",
    "        S = torch.masked_fill(S, mask.unsqueeze(1), 0)\n",
    "        X = (S @ V).permute(0, 2, 1, 3).reshape(N, L, D)\n",
    "        X = self.intermediate_norm((X + input).reshape(-1, D)).reshape(N, L, D)\n",
    "        intermediate = X\n",
    "        X = self.expand(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.contract(X)\n",
    "        X = self.out_norm((X + intermediate).reshape(-1, D)).reshape(N, L, D)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes: int, tokenizer: Tokenizer, n_stages: int, projection_dim: int, n_heads: int, activation: Callable = torch.relu):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.vocabulary), n_heads*projection_dim)\n",
    "        self.stages = torch.nn.ModuleList()\n",
    "        for _ in range(n_stages):\n",
    "            self.stages.append(AttentionBlock(projection_dim, n_heads, activation))\n",
    "        self.output = torch.nn.Linear(projection_dim * n_heads, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            tensor of shape (N, L)\n",
    "        \"\"\"\n",
    "        X = X.to(self.device)\n",
    "        mask = (X == self.tokenizer.PAD)\n",
    "        mask = (mask.unsqueeze(1) | mask.unsqueeze(2))\n",
    "        X = self.embedding(X)\n",
    "        for stage in self.stages:\n",
    "            if self.training:\n",
    "                X = checkpoint(stage, X, mask)\n",
    "            else:\n",
    "                X = stage(X, mask)\n",
    "        return self.output(X.mean(dim=1))\n",
    "    \n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Y = self(X)\n",
    "        return Y.max(dim=1).indices\n",
    "    \n",
    "    def loss(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self(X)\n",
    "        return F.cross_entropy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    def metric(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(labels), tokenizer, 4, 16, 8)\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0E-3)\n",
    "train = Batchifyer(df_train, tokenizer, n_batches=1, batch_size=None)\n",
    "val = Batchifyer(df_val, tokenizer, n_batches=1, batch_size=None)\n",
    "train_loop(model, optimizer, train, val, n_steps=1000, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_to_tensor(df_test, tokenizer)\n",
    "y_pred = model.predict(X)\n",
    "acc = accuracy(y_pred, Y.to(y_pred.device))\n",
    "print(f\"accuracy {acc:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2017a09ab1f54c7a6a1af190715fd60264df4a93389a277e03c18d947a6e489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

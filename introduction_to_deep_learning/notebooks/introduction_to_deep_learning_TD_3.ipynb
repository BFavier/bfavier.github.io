{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import Callable, Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialToken:\n",
    "    \"\"\"\n",
    "    A special token for tokenizers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, string: str):\n",
    "        self.string = string.upper()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.string}>\"\n",
    "    \n",
    "    def __equal__(self, other) -> bool:\n",
    "        if isinstance(other, SpecialToken):\n",
    "            return self.string == other.string\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.string)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    A simple word tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    word_pattern = re.compile(R\"\\w+|\\d+|[^\\w\\d\\s]\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tokenizer({len(self.vocabulary)} tokens)\"\n",
    "\n",
    "    def __init__(self, corpus: Iterable[str], min_frequency: float = 1.0E-6):\n",
    "        words = [word for document in corpus for word in self._split(document)]\n",
    "        word_counts = Counter(words)\n",
    "        word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.vocabulary = [k for k, v in word_counts if v/len(words) >= min_frequency] + [SpecialToken(\"UNKNOWN\"), SpecialToken(\"END\"), SpecialToken(\"PAD\")]\n",
    "        self.map = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def split(self, document: str) -> list[str]:\n",
    "        return self.decode(self.encode(document))\n",
    "\n",
    "    def encode(self, document: str) -> list[int]:\n",
    "        return [self.map.get(word, SpecialToken(\"UNKNOWN\")) for word in self._split(document)]\n",
    "\n",
    "    def decode(self, encoded: list[int]) -> list[str]:\n",
    "        return [i.string if isinstance(i, SpecialToken) else self.vocabulary[i] for i in encoded]\n",
    "\n",
    "    def _split(self, document: str) -> list[str]:\n",
    "        return self.word_pattern.findall(document)\n",
    "\n",
    "    @property\n",
    "    def PAD(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"PAD\")]\n",
    "    \n",
    "    @property\n",
    "    def END(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"END\")]\n",
    "    \n",
    "    @property\n",
    "    def UNKNOWN(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"UNKNOWN\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'airline_sentiment'], dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/Twitter_US_Airline_Sentiment.csv\")\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(4314 tokens)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(df.text, min_frequency=1.0E-5)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USAirways how can i get ahold of a reservations supervisor?\n",
      "['@', 'USAirways', 'how', 'can', 'i', 'get', 'ahold', 'of', 'a', 'reservations', 'supervisor', '?']\n",
      "\n",
      "@VirginAmerica When will VX use all 6 LGA slots instead of 4 today? Adding AUS makes this less likely :(\n",
      "['@', 'VirginAmerica', 'When', 'will', 'VX', 'use', 'all', '6', 'LGA', 'UNKNOWN', 'instead', 'of', '4', 'today', '?', 'UNKNOWN', 'AUS', 'makes', 'this', 'less', 'likely', ':', '(']\n",
      "\n",
      "@AmericanAir that's 16+ extra hours of travel time. Missed vacation time and now you guys are messing with my professional life.\n",
      "['@', 'AmericanAir', 'that', \"'\", 's', '16', '+', 'extra', 'hours', 'of', 'travel', 'time', '.', 'Missed', 'vacation', 'time', 'and', 'now', 'you', 'guys', 'are', 'messing', 'with', 'my', 'professional', 'life', '.']\n",
      "\n",
      "@SouthwestAir Thanks I just sent a DM with this info.\n",
      "['@', 'SouthwestAir', 'Thanks', 'I', 'just', 'sent', 'a', 'DM', 'with', 'this', 'info', '.']\n",
      "\n",
      "@USAirways horrible travel day w/ your airlines. My 2:20 flight to Birmingham was Cancelled Flightled w/ no notification so I couldn't get on the 4:05\n",
      "['@', 'USAirways', 'horrible', 'travel', 'day', 'w', '/', 'your', 'airlines', '.', 'My', '2', ':', '20', 'flight', 'to', 'Birmingham', 'was', 'Cancelled', 'Flightled', 'w', '/', 'no', 'notification', 'so', 'I', 'couldn', \"'\", 't', 'get', 'on', 'the', '4', ':', '05']\n",
      "\n",
      "@JetBlue I would like to send an email to Lost and Found at JetBlue JFK; I spoke to them this morning but cannot drive their today.\n",
      "['@', 'JetBlue', 'I', 'would', 'like', 'to', 'send', 'an', 'email', 'to', 'Lost', 'and', 'Found', 'at', 'JetBlue', 'JFK', ';', 'I', 'spoke', 'to', 'them', 'this', 'morning', 'but', 'cannot', 'drive', 'their', 'today', '.']\n",
      "\n",
      "@SouthwestAir yall still fly in the cold right?\n",
      "['@', 'SouthwestAir', 'yall', 'still', 'fly', 'in', 'the', 'cold', 'right', '?']\n",
      "\n",
      "@AmericanAir Nope. Couldn't make changes online and after 90 mins on hold and time dealing w/ the rude rep, the 24 hour window has closed.\n",
      "['@', 'AmericanAir', 'Nope', '.', 'Couldn', \"'\", 't', 'make', 'changes', 'online', 'and', 'after', '90', 'mins', 'on', 'hold', 'and', 'time', 'dealing', 'w', '/', 'the', 'rude', 'rep', ',', 'the', '24', 'hour', 'window', 'has', 'closed', '.']\n",
      "\n",
      "@SouthwestAir no worries. We got thru eventually. I was just curious. Best of luck to you dealing with the weather!\n",
      "['@', 'SouthwestAir', 'no', 'worries', '.', 'We', 'got', 'thru', 'eventually', '.', 'I', 'was', 'just', 'curious', '.', 'Best', 'of', 'luck', 'to', 'you', 'dealing', 'with', 'the', 'weather', '!']\n",
      "\n",
      "@SouthwestAir trying to fly out of Nashville tomorrow. How is it looking?\n",
      "['@', 'SouthwestAir', 'trying', 'to', 'fly', 'out', 'of', 'Nashville', 'tomorrow', '.', 'How', 'is', 'it', 'looking', '?']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in df.sample(n=10).text:\n",
    "    print(text)\n",
    "    print(tokenizer.split(text))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted: torch.Tensor, target: torch.Tensor):\n",
    "    assert target.shape == predicted.shape\n",
    "    assert target.dtype == torch.long\n",
    "    assert predicted.dtype == torch.long\n",
    "    with torch.no_grad():\n",
    "        return torch.mean((predicted == target).float()).cpu().item()\n",
    "\n",
    "\n",
    "def input_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> torch.Tensor:\n",
    "    encoded = [tokenizer.encode(document) for document in df['text']]\n",
    "    L = max([len(doc) for doc in encoded])\n",
    "    encoded = [doc + [tokenizer.PAD]*(L - len(doc)) for doc in encoded]\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "def target_to_tensor():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "\n",
    "Implémenter et entraîner un réseau récurrent pour classifier les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, tokenizer: Tokenizer, in_features: int, hidden_state_features: int, activation: Callable = torch.relu):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_state_features = hidden_state_features\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.vocabulary), in_features)\n",
    "        self.linear = torch.nn.Linear(in_features + hidden_state_features, in_features + hidden_state_features)\n",
    "        self.activation = activation\n",
    "        self.output = torch.nn.Linear(in_features + hidden_state_features, in_features + hidden_state_features)\n",
    "        self.normalization = torch.nn.LayerNorm(hidden_state_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : torch.Tensor\n",
    "            tensor of long of shape (N, L)\n",
    "        H : torch.Tensor\n",
    "            tensor of float of shape (N, D)\n",
    "        \"\"\"\n",
    "        N, L = X.shape\n",
    "        H = torch.zeros((N, self.hidden_state_features), dtype=torch.float32, device=X.device)\n",
    "        for x in X.transpose(0, 1):\n",
    "            I = self.embedding(x)\n",
    "            T = torch.cat([I, H], dim=1)\n",
    "            T = self.linear(T)\n",
    "            T = self.activation(T)\n",
    "            T = self.output(T)\n",
    "            H = torch.where(x == self.tokenizer.PAD, H, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2017a09ab1f54c7a6a1af190715fd60264df4a93389a277e03c18d947a6e489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from typing import Callable, Iterable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialToken:\n",
    "    \"\"\"\n",
    "    A special token for tokenizers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, string: str):\n",
    "        self.string = string.upper()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.string}>\"\n",
    "    \n",
    "    def __eq__(self, other) -> bool:\n",
    "        if isinstance(other, SpecialToken):\n",
    "            return self.string == other.string\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.string)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    A simple word tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    word_pattern = re.compile(R\"\\w+|\\d+|[^\\w\\d\\s]\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tokenizer({len(self.vocabulary)} tokens)\"\n",
    "\n",
    "    def __init__(self, corpus: Iterable[str], min_frequency: float = 1.0E-6):\n",
    "        words = [word for document in corpus for word in self._split(document)]\n",
    "        word_counts = Counter(words)\n",
    "        word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.vocabulary = [k for k, v in word_counts if v/len(words) >= min_frequency] + [SpecialToken(\"UNKNOWN\"), SpecialToken(\"END\"), SpecialToken(\"PAD\")]\n",
    "        self.map = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def split(self, document: str) -> list[str]:\n",
    "        return self.decode(self.encode(document))\n",
    "\n",
    "    def encode(self, document: str) -> list[int]:\n",
    "        return [self.map.get(word, self.UNKNOWN) for word in self._split(document)]\n",
    "\n",
    "    def decode(self, encoded: list[int]) -> list[str]:\n",
    "        return [i.string if isinstance(i, SpecialToken) else self.vocabulary[i] for i in encoded]\n",
    "\n",
    "    def _split(self, document: str) -> list[str]:\n",
    "        return self.word_pattern.findall(document)\n",
    "\n",
    "    @property\n",
    "    def PAD(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"PAD\")]\n",
    "    \n",
    "    @property\n",
    "    def END(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"END\")]\n",
    "    \n",
    "    @property\n",
    "    def UNKNOWN(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"UNKNOWN\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'airline_sentiment'], dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/Twitter_US_Airline_Sentiment.csv\")\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(4314 tokens)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(df.text, min_frequency=1.0E-5)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@united we had four scheduled flights on this reservation and literally did not take one! Unreal\n",
      "['@', 'united', 'we', 'had', 'four', 'scheduled', 'flights', 'on', 'this', 'reservation', 'and', 'literally', 'did', 'not', 'take', 'one', '!', 'Unreal']\n",
      "\n",
      "@united THAT'S the tweet u choose to answer, to tell me you're not liable?! #youretheworst #neveragain\n",
      "['@', 'united', 'THAT', \"'\", 'S', 'the', 'tweet', 'u', 'choose', 'to', 'answer', ',', 'to', 'tell', 'me', 'you', \"'\", 're', 'not', <UNKNOWN>, '?', '!', '#', <UNKNOWN>, '#', 'neveragain']\n",
      "\n",
      "@USAirways will never travel with you again. This is insanity. storms are inevitable but making us hold to just add a lap child #badservice\n",
      "['@', 'USAirways', 'will', 'never', 'travel', 'with', 'you', 'again', '.', 'This', 'is', <UNKNOWN>, '.', 'storms', 'are', <UNKNOWN>, 'but', 'making', 'us', 'hold', 'to', 'just', 'add', 'a', 'lap', 'child', '#', 'badservice']\n",
      "\n",
      "@jetblue I missed a goal On @NHLonNBCSports because of your Inept male stewardess. I should have landed already but I havent taken off yet.\n",
      "['@', 'jetblue', 'I', 'missed', 'a', <UNKNOWN>, 'On', '@', <UNKNOWN>, 'because', 'of', 'your', <UNKNOWN>, 'male', 'stewardess', '.', 'I', 'should', 'have', 'landed', 'already', 'but', 'I', 'havent', 'taken', 'off', 'yet', '.']\n",
      "\n",
      "@USAirways - delayed 1851. Need an extra 5 min to make 1769. Would appreciate a little help again this week.\n",
      "['@', 'USAirways', '-', 'delayed', <UNKNOWN>, '.', 'Need', 'an', 'extra', '5', 'min', 'to', 'make', <UNKNOWN>, '.', 'Would', 'appreciate', 'a', 'little', 'help', 'again', 'this', 'week', '.']\n",
      "\n",
      "@united I'm so frustrated and nervous because of this.\n",
      "['@', 'united', 'I', \"'\", 'm', 'so', 'frustrated', 'and', 'nervous', 'because', 'of', 'this', '.']\n",
      "\n",
      "@AmericanAir thx for responding. I cant watch 2 mins of this film w/out it cutting in and out 4 prolonged prds of time. beyond frustrating\n",
      "['@', 'AmericanAir', 'thx', 'for', 'responding', '.', 'I', 'cant', 'watch', '2', 'mins', 'of', 'this', 'film', 'w', '/', 'out', 'it', 'cutting', 'in', 'and', 'out', '4', <UNKNOWN>, <UNKNOWN>, 'of', 'time', '.', 'beyond', 'frustrating']\n",
      "\n",
      "@SouthwestAir yeah, all 4 are rebooked for tomorrow AM. Hoping the 3rd time is a charm for no Cancelled Flightled flights!\n",
      "['@', 'SouthwestAir', 'yeah', ',', 'all', '4', 'are', 'rebooked', 'for', 'tomorrow', 'AM', '.', 'Hoping', 'the', '3rd', 'time', 'is', 'a', <UNKNOWN>, 'for', 'no', 'Cancelled', 'Flightled', 'flights', '!']\n",
      "\n",
      "............. RT @JetBlue: Our fleet's on fleek. http://t.co/X7iLzqdwe2\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'RT', '@', 'JetBlue', ':', 'Our', 'fleet', \"'\", 's', 'on', 'fleek', '.', 'http', ':', '/', '/', 't', '.', 'co', '/', <UNKNOWN>]\n",
      "\n",
      "@united could really use your help getting our bags at IAD. We're headed to YOW but now going to YXU\n",
      "['@', 'united', 'could', 'really', 'use', 'your', 'help', 'getting', 'our', 'bags', 'at', 'IAD', '.', 'We', \"'\", 're', 'headed', 'to', <UNKNOWN>, 'but', 'now', 'going', 'to', <UNKNOWN>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in df.sample(n=10).text:\n",
    "    print(text)\n",
    "    print(tokenizer.split(text))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7)\n",
    "df = df.drop(index=df_train.index)\n",
    "df_val = df.sample(frac=0.5)\n",
    "df_test = df.drop(index=df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted: torch.Tensor, target: torch.Tensor):\n",
    "    assert target.shape == predicted.shape\n",
    "    assert target.dtype == torch.long\n",
    "    assert predicted.dtype == torch.long\n",
    "    with torch.no_grad():\n",
    "        return torch.mean((predicted == target).float()).cpu().item()\n",
    "\n",
    "\n",
    "def input_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> torch.Tensor:\n",
    "    encoded = [tokenizer.encode(document) for document in df['text']]\n",
    "    L = max([len(doc) for doc in encoded])\n",
    "    encoded = [doc + [tokenizer.PAD]*(L - len(doc)) for doc in encoded]\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "def target_to_tensor(df: pd.DataFrame) -> torch.Tensor:\n",
    "    map = {k: v for v, k in enumerate(labels)}\n",
    "    return torch.tensor([map[label] for label in df[\"airline_sentiment\"]], dtype=torch.long)\n",
    "\n",
    "\n",
    "def data_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> tuple[torch.Tensor]:\n",
    "    return (input_to_tensor(df, tokenizer), target_to_tensor(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batchifyer:\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: Tokenizer, n_batches: int, batch_size: Optional[int]):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_batches = n_batches\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        shuffled = df.sample(frac=1.)\n",
    "        return (self._batch(shuffled, i) for i in range(self.n_batches))\n",
    "    \n",
    "    def _batch(self, shuffled: pd.DataFrame, i: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = self.batch_size or len(shuffled) // self.n_batches\n",
    "        subset = shuffled.iloc[i*batch_size:(i+1)*batch_size]\n",
    "        return data_to_tensor(subset, self.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_data: Iterable[tuple[torch.Tensor]], val_data: Iterable[tuple[torch.Tensor]], n_steps: int = 1000, patience: int = 100, keep_best: bool = True):\n",
    "    \"\"\"\n",
    "    train the model for the specified number of steps, or untilearly stopping\n",
    "    \"\"\"\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_step = 0\n",
    "    best_metric = 0.\n",
    "    try:\n",
    "        for step in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "            # train loss\n",
    "            model.train()\n",
    "            losses = []\n",
    "            for x, y in train_data:\n",
    "                loss = model.loss(x, y)\n",
    "                loss.backward()\n",
    "                losses.append(loss.item())\n",
    "            loss = sum(losses)/len(losses)\n",
    "            # val metric\n",
    "            model.eval()\n",
    "            metrics = []\n",
    "            for x, y in val_data:\n",
    "                metrics.append(model.metric(x, y))\n",
    "            metric = sum(metrics) / len(metrics)\n",
    "            # checkpointing\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_step = step\n",
    "                if keep_best:\n",
    "                    best_state = deepcopy(model.state_dict())\n",
    "            elif step - best_step > patience:\n",
    "                print(\"early stoping\")\n",
    "                break\n",
    "            # optimizer steping\n",
    "            optimizer.step()\n",
    "            # printing\n",
    "            print(f\"Step {step}: loss = {loss:.3g} metric = {metric:.2%}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted by user\")\n",
    "    if keep_best:\n",
    "        model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "\n",
    "Implémenter et entraîner un réseau récurrent pour classifier les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes: int, tokenizer: Tokenizer, in_features: int, hidden_state_features: int, activation: Callable = torch.relu):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_state_features = hidden_state_features\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.vocabulary), in_features)\n",
    "        self.linear = torch.nn.Linear(in_features + hidden_state_features, in_features + hidden_state_features)\n",
    "        self.activation = activation\n",
    "        self.contract = torch.nn.Linear(in_features + hidden_state_features, hidden_state_features)\n",
    "        self.normalization = torch.nn.LayerNorm(hidden_state_features)\n",
    "        self.output = torch.nn.Linear(hidden_state_features, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : torch.Tensor\n",
    "            tensor of long of shape (N, L)\n",
    "        H : torch.Tensor\n",
    "            tensor of float of shape (N, D)\n",
    "        \"\"\"\n",
    "        X = X.to(self.device)\n",
    "        N, L = X.shape\n",
    "        H = torch.zeros((N, self.hidden_state_features), dtype=torch.float32, device=X.device)\n",
    "        for x in X.transpose(0, 1):\n",
    "            I = self.embedding(x)\n",
    "            T = torch.cat([I, H], dim=1)\n",
    "            T = self.linear(T)\n",
    "            T = self.activation(T)\n",
    "            T = self.contract(T)\n",
    "            H = torch.where(x.unsqueeze(1) == self.tokenizer.PAD, H, T)\n",
    "        return self.output(H)\n",
    "    \n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Y = self(X)\n",
    "        return Y.max(dim=1).indices\n",
    "    \n",
    "    def loss(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self(X)\n",
    "        return F.cross_entropy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    def metric(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.08 metric = 47.06%\n",
      "Step 1: loss = 0.997 metric = 62.07%\n",
      "Step 2: loss = 0.939 metric = 62.09%\n",
      "Step 3: loss = 0.909 metric = 62.09%\n",
      "Step 4: loss = 0.907 metric = 62.09%\n",
      "Step 5: loss = 0.914 metric = 62.09%\n",
      "Step 6: loss = 0.909 metric = 62.09%\n",
      "Step 7: loss = 0.896 metric = 62.09%\n",
      "Step 8: loss = 0.883 metric = 62.09%\n",
      "Step 9: loss = 0.875 metric = 62.09%\n",
      "Step 10: loss = 0.873 metric = 62.09%\n",
      "Step 11: loss = 0.872 metric = 62.09%\n",
      "Step 12: loss = 0.872 metric = 62.09%\n",
      "Step 13: loss = 0.87 metric = 62.45%\n",
      "Step 14: loss = 0.866 metric = 62.57%\n",
      "Step 15: loss = 0.861 metric = 62.45%\n",
      "Step 16: loss = 0.856 metric = 62.16%\n",
      "Step 17: loss = 0.852 metric = 62.09%\n",
      "Step 18: loss = 0.85 metric = 62.09%\n",
      "Step 19: loss = 0.849 metric = 62.09%\n",
      "Step 20: loss = 0.847 metric = 62.52%\n",
      "Step 21: loss = 0.844 metric = 62.57%\n",
      "Step 22: loss = 0.839 metric = 62.68%\n",
      "Step 23: loss = 0.835 metric = 62.59%\n",
      "Step 24: loss = 0.832 metric = 62.57%\n",
      "Step 25: loss = 0.829 metric = 63.11%\n",
      "Step 26: loss = 0.827 metric = 63.43%\n",
      "Step 27: loss = 0.823 metric = 63.46%\n",
      "Step 28: loss = 0.819 metric = 63.50%\n",
      "Step 29: loss = 0.815 metric = 63.41%\n",
      "Step 30: loss = 0.81 metric = 63.55%\n",
      "Step 31: loss = 0.807 metric = 63.64%\n",
      "Step 32: loss = 0.803 metric = 63.84%\n",
      "Step 33: loss = 0.797 metric = 64.39%\n",
      "Step 34: loss = 0.791 metric = 64.69%\n",
      "Step 35: loss = 0.785 metric = 65.16%\n",
      "Step 36: loss = 0.779 metric = 65.55%\n",
      "Step 37: loss = 0.772 metric = 65.66%\n",
      "Step 38: loss = 0.765 metric = 66.21%\n",
      "Step 39: loss = 0.757 metric = 66.80%\n",
      "Step 40: loss = 0.747 metric = 67.17%\n",
      "Step 41: loss = 0.742 metric = 67.58%\n",
      "Step 42: loss = 0.777 metric = 66.62%\n",
      "Step 43: loss = 0.735 metric = 68.28%\n",
      "Step 44: loss = 0.773 metric = 67.28%\n",
      "Step 45: loss = 0.723 metric = 68.49%\n",
      "Step 46: loss = 0.738 metric = 68.47%\n",
      "Step 47: loss = 0.743 metric = 68.37%\n",
      "Step 48: loss = 0.736 metric = 68.40%\n",
      "Step 49: loss = 0.727 metric = 68.26%\n",
      "Step 50: loss = 0.723 metric = 67.81%\n",
      "Step 51: loss = 0.723 metric = 67.96%\n",
      "Step 52: loss = 0.718 metric = 68.17%\n",
      "Step 53: loss = 0.709 metric = 68.76%\n",
      "Step 54: loss = 0.703 metric = 69.29%\n",
      "Step 55: loss = 0.7 metric = 69.72%\n",
      "Step 56: loss = 0.694 metric = 69.74%\n",
      "Step 57: loss = 0.683 metric = 69.92%\n",
      "Step 58: loss = 0.674 metric = 70.08%\n",
      "Step 59: loss = 0.67 metric = 70.42%\n",
      "Step 60: loss = 0.656 metric = 71.22%\n",
      "Step 61: loss = 0.647 metric = 71.68%\n",
      "Step 62: loss = 0.634 metric = 72.45%\n",
      "Step 63: loss = 0.624 metric = 72.97%\n",
      "Step 64: loss = 0.61 metric = 73.91%\n",
      "Step 65: loss = 0.602 metric = 74.29%\n",
      "Step 66: loss = 0.607 metric = 74.50%\n",
      "Step 67: loss = 0.65 metric = 71.17%\n",
      "Step 68: loss = 0.595 metric = 74.95%\n",
      "Step 69: loss = 0.609 metric = 73.57%\n",
      "Step 70: loss = 0.58 metric = 76.43%\n",
      "Step 71: loss = 0.593 metric = 75.98%\n",
      "Step 72: loss = 0.568 metric = 76.41%\n",
      "Step 73: loss = 0.616 metric = 74.39%\n",
      "Step 74: loss = 0.591 metric = 75.02%\n",
      "Step 75: loss = 0.615 metric = 74.89%\n",
      "Step 76: loss = 0.575 metric = 76.66%\n",
      "Step 77: loss = 0.56 metric = 75.57%\n",
      "Step 78: loss = 0.595 metric = 73.63%\n",
      "Step 79: loss = 0.544 metric = 76.46%\n",
      "Step 80: loss = 0.531 metric = 78.57%\n",
      "Step 81: loss = 0.538 metric = 77.64%\n",
      "Step 82: loss = 0.488 metric = 78.89%\n",
      "Step 83: loss = 0.582 metric = 77.78%\n",
      "Step 84: loss = 0.585 metric = 74.68%\n",
      "Step 85: loss = 0.553 metric = 77.44%\n",
      "Step 86: loss = 0.525 metric = 79.94%\n",
      "Step 87: loss = 0.544 metric = 77.41%\n",
      "Step 88: loss = 0.568 metric = 75.34%\n",
      "Step 89: loss = 0.545 metric = 75.98%\n",
      "Step 90: loss = 0.509 metric = 78.30%\n",
      "Step 91: loss = 0.506 metric = 79.80%\n",
      "Step 92: loss = 0.518 metric = 78.39%\n",
      "Step 93: loss = 0.506 metric = 78.73%\n",
      "Step 94: loss = 0.473 metric = 80.40%\n",
      "Step 95: loss = 0.482 metric = 79.94%\n",
      "Step 96: loss = 0.454 metric = 81.53%\n",
      "Step 97: loss = 0.441 metric = 81.31%\n",
      "Step 98: loss = 0.441 metric = 81.67%\n",
      "Step 99: loss = 0.409 metric = 83.56%\n",
      "Step 100: loss = 0.435 metric = 81.79%\n",
      "Step 101: loss = 0.413 metric = 83.70%\n",
      "Step 102: loss = 0.419 metric = 83.36%\n",
      "Step 103: loss = 0.386 metric = 84.77%\n",
      "Step 104: loss = 0.41 metric = 83.97%\n",
      "Step 105: loss = 0.39 metric = 84.47%\n",
      "Step 106: loss = 0.375 metric = 85.36%\n",
      "Step 107: loss = 0.371 metric = 84.86%\n",
      "Step 108: loss = 0.371 metric = 84.93%\n",
      "Step 109: loss = 0.36 metric = 86.04%\n",
      "Step 110: loss = 0.356 metric = 86.41%\n",
      "Step 111: loss = 0.335 metric = 86.68%\n",
      "Step 112: loss = 0.343 metric = 86.20%\n",
      "Step 113: loss = 0.339 metric = 86.25%\n",
      "Step 114: loss = 0.314 metric = 87.77%\n",
      "Step 115: loss = 0.324 metric = 87.30%\n",
      "Step 116: loss = 0.311 metric = 88.41%\n",
      "Step 117: loss = 0.297 metric = 89.03%\n",
      "Step 118: loss = 0.297 metric = 88.41%\n",
      "Step 119: loss = 0.292 metric = 88.73%\n",
      "Step 120: loss = 0.275 metric = 89.71%\n",
      "Step 121: loss = 0.277 metric = 89.66%\n",
      "Step 122: loss = 0.27 metric = 90.12%\n",
      "Step 123: loss = 0.257 metric = 90.57%\n",
      "Step 124: loss = 0.256 metric = 90.07%\n",
      "Step 125: loss = 0.25 metric = 90.37%\n",
      "Step 126: loss = 0.239 metric = 91.28%\n",
      "Step 127: loss = 0.239 metric = 91.39%\n",
      "Step 128: loss = 0.231 metric = 91.92%\n",
      "Step 129: loss = 0.221 metric = 92.28%\n",
      "Step 130: loss = 0.221 metric = 92.33%\n",
      "Step 131: loss = 0.21 metric = 92.76%\n",
      "Step 132: loss = 0.209 metric = 92.96%\n",
      "Step 133: loss = 0.206 metric = 92.92%\n",
      "Step 134: loss = 0.207 metric = 92.83%\n",
      "Step 135: loss = 0.238 metric = 90.92%\n",
      "Step 136: loss = 0.562 metric = 76.30%\n",
      "Step 137: loss = 0.259 metric = 90.51%\n",
      "Step 138: loss = 0.617 metric = 77.28%\n",
      "Step 139: loss = 0.407 metric = 83.27%\n",
      "Step 140: loss = 0.542 metric = 76.34%\n",
      "Step 141: loss = 0.553 metric = 76.32%\n",
      "Step 142: loss = 0.514 metric = 78.26%\n",
      "Step 143: loss = 0.486 metric = 79.42%\n",
      "Step 144: loss = 0.49 metric = 79.51%\n",
      "Step 145: loss = 0.51 metric = 78.89%\n",
      "Step 146: loss = 0.512 metric = 78.39%\n",
      "Step 147: loss = 0.489 metric = 79.14%\n",
      "Step 148: loss = 0.453 metric = 80.58%\n",
      "Step 149: loss = 0.43 metric = 81.81%\n",
      "Step 150: loss = 0.426 metric = 82.70%\n",
      "Step 151: loss = 0.421 metric = 83.13%\n",
      "Step 152: loss = 0.397 metric = 84.38%\n",
      "Step 153: loss = 0.358 metric = 86.00%\n",
      "Step 154: loss = 0.323 metric = 87.43%\n",
      "Step 155: loss = 0.304 metric = 87.27%\n",
      "Step 156: loss = 0.299 metric = 87.30%\n",
      "Step 157: loss = 0.317 metric = 87.55%\n",
      "Step 158: loss = 0.272 metric = 88.98%\n",
      "Step 159: loss = 0.228 metric = 91.71%\n",
      "Step 160: loss = 0.225 metric = 93.17%\n",
      "Step 161: loss = 0.234 metric = 92.44%\n",
      "Step 162: loss = 0.215 metric = 92.96%\n",
      "Step 163: loss = 0.203 metric = 93.03%\n",
      "Step 164: loss = 0.2 metric = 92.85%\n",
      "Step 165: loss = 0.216 metric = 91.23%\n",
      "Step 166: loss = 0.186 metric = 93.24%\n",
      "Step 167: loss = 0.182 metric = 93.67%\n",
      "Step 168: loss = 0.179 metric = 93.85%\n",
      "Step 169: loss = 0.179 metric = 93.99%\n",
      "Step 170: loss = 0.165 metric = 94.49%\n",
      "Step 171: loss = 0.154 metric = 94.92%\n",
      "Step 172: loss = 0.155 metric = 94.76%\n",
      "Step 173: loss = 0.156 metric = 94.67%\n",
      "Step 174: loss = 0.143 metric = 95.49%\n",
      "Step 175: loss = 0.141 metric = 95.79%\n",
      "Step 176: loss = 0.141 metric = 95.67%\n",
      "Step 177: loss = 0.137 metric = 95.99%\n",
      "Step 178: loss = 0.129 metric = 96.31%\n",
      "Step 179: loss = 0.127 metric = 96.24%\n",
      "Step 180: loss = 0.126 metric = 95.88%\n",
      "Step 181: loss = 0.118 metric = 96.63%\n",
      "Step 182: loss = 0.116 metric = 96.45%\n",
      "Step 183: loss = 0.114 metric = 96.74%\n",
      "Step 184: loss = 0.11 metric = 96.81%\n",
      "Step 185: loss = 0.106 metric = 96.99%\n",
      "Step 186: loss = 0.106 metric = 96.99%\n",
      "Step 187: loss = 0.1 metric = 97.15%\n",
      "Step 188: loss = 0.0985 metric = 97.27%\n",
      "Step 189: loss = 0.0955 metric = 97.38%\n",
      "Step 190: loss = 0.0939 metric = 97.50%\n",
      "Step 191: loss = 0.0904 metric = 97.63%\n",
      "Step 192: loss = 0.0893 metric = 97.70%\n",
      "Step 193: loss = 0.0859 metric = 97.81%\n",
      "Step 194: loss = 0.084 metric = 97.91%\n",
      "Step 195: loss = 0.0819 metric = 97.95%\n",
      "Step 196: loss = 0.0799 metric = 98.00%\n",
      "Step 197: loss = 0.0774 metric = 98.04%\n",
      "Step 198: loss = 0.0755 metric = 98.11%\n",
      "Step 199: loss = 0.0734 metric = 98.18%\n",
      "Step 200: loss = 0.0714 metric = 98.20%\n",
      "Step 201: loss = 0.0698 metric = 98.20%\n",
      "Step 202: loss = 0.0681 metric = 98.32%\n",
      "Step 203: loss = 0.0663 metric = 98.41%\n",
      "Step 204: loss = 0.0649 metric = 98.47%\n",
      "Step 205: loss = 0.063 metric = 98.47%\n",
      "Step 206: loss = 0.0616 metric = 98.47%\n",
      "Step 207: loss = 0.0602 metric = 98.52%\n",
      "Step 208: loss = 0.0586 metric = 98.59%\n",
      "Step 209: loss = 0.0574 metric = 98.61%\n",
      "Step 210: loss = 0.0558 metric = 98.72%\n",
      "Step 211: loss = 0.0546 metric = 98.72%\n",
      "Step 212: loss = 0.0534 metric = 98.77%\n",
      "Step 213: loss = 0.0522 metric = 98.82%\n",
      "Step 214: loss = 0.051 metric = 98.84%\n",
      "Step 215: loss = 0.0498 metric = 98.88%\n",
      "Step 216: loss = 0.0487 metric = 98.88%\n",
      "Step 217: loss = 0.0476 metric = 98.91%\n",
      "Step 218: loss = 0.0466 metric = 98.95%\n",
      "Step 219: loss = 0.0456 metric = 98.98%\n",
      "Step 220: loss = 0.0447 metric = 99.00%\n",
      "Step 221: loss = 0.0437 metric = 99.04%\n",
      "Step 222: loss = 0.0427 metric = 99.02%\n",
      "Step 223: loss = 0.0418 metric = 99.02%\n",
      "Step 224: loss = 0.0408 metric = 99.02%\n",
      "Step 225: loss = 0.0398 metric = 99.07%\n",
      "Step 226: loss = 0.0389 metric = 99.11%\n",
      "Step 227: loss = 0.0381 metric = 99.13%\n",
      "Step 228: loss = 0.0373 metric = 99.16%\n",
      "Step 229: loss = 0.0366 metric = 99.20%\n",
      "Step 230: loss = 0.0359 metric = 99.20%\n",
      "Step 231: loss = 0.0352 metric = 99.20%\n",
      "Step 232: loss = 0.0346 metric = 99.23%\n",
      "Step 233: loss = 0.0341 metric = 99.23%\n",
      "Step 234: loss = 0.0335 metric = 99.23%\n",
      "Step 235: loss = 0.033 metric = 99.25%\n",
      "Step 236: loss = 0.0325 metric = 99.27%\n",
      "Step 237: loss = 0.0319 metric = 99.29%\n",
      "Step 238: loss = 0.0315 metric = 99.34%\n",
      "Step 239: loss = 0.031 metric = 99.36%\n",
      "Step 240: loss = 0.0305 metric = 99.36%\n",
      "Step 241: loss = 0.0301 metric = 99.36%\n",
      "Step 242: loss = 0.0297 metric = 99.36%\n",
      "Step 243: loss = 0.0292 metric = 99.36%\n",
      "Step 244: loss = 0.0288 metric = 99.36%\n",
      "Step 245: loss = 0.0283 metric = 99.36%\n",
      "Step 246: loss = 0.0279 metric = 99.39%\n",
      "Step 247: loss = 0.0273 metric = 99.39%\n",
      "Step 248: loss = 0.0267 metric = 99.39%\n",
      "Step 249: loss = 0.0262 metric = 99.41%\n",
      "Step 250: loss = 0.0259 metric = 99.41%\n",
      "Step 251: loss = 0.0255 metric = 99.41%\n",
      "Step 252: loss = 0.0252 metric = 99.43%\n",
      "Step 253: loss = 0.0248 metric = 99.43%\n",
      "Step 254: loss = 0.0245 metric = 99.43%\n",
      "Step 255: loss = 0.0241 metric = 99.43%\n",
      "Step 256: loss = 0.0238 metric = 99.43%\n",
      "Step 257: loss = 0.0235 metric = 99.43%\n",
      "Step 258: loss = 0.0231 metric = 99.45%\n",
      "Step 259: loss = 0.0228 metric = 99.45%\n",
      "Step 260: loss = 0.0224 metric = 99.45%\n",
      "Step 261: loss = 0.0221 metric = 99.45%\n",
      "Step 262: loss = 0.0217 metric = 99.45%\n",
      "Step 263: loss = 0.0214 metric = 99.45%\n",
      "Step 264: loss = 0.021 metric = 99.52%\n",
      "Step 265: loss = 0.0207 metric = 99.50%\n",
      "Step 266: loss = 0.0204 metric = 99.54%\n",
      "Step 267: loss = 0.0201 metric = 99.57%\n",
      "Step 268: loss = 0.0197 metric = 99.54%\n",
      "Step 269: loss = 0.0194 metric = 99.54%\n",
      "Step 270: loss = 0.0192 metric = 99.54%\n",
      "Step 271: loss = 0.0189 metric = 99.54%\n",
      "Step 272: loss = 0.0186 metric = 99.54%\n",
      "Step 273: loss = 0.0184 metric = 99.57%\n",
      "Step 274: loss = 0.0181 metric = 99.54%\n",
      "Step 275: loss = 0.0179 metric = 99.54%\n",
      "Step 276: loss = 0.0176 metric = 99.54%\n",
      "Step 277: loss = 0.0174 metric = 99.54%\n",
      "Step 278: loss = 0.0171 metric = 99.54%\n",
      "Step 279: loss = 0.0169 metric = 99.54%\n",
      "Step 280: loss = 0.0167 metric = 99.54%\n",
      "Step 281: loss = 0.0165 metric = 99.57%\n",
      "Step 282: loss = 0.0162 metric = 99.57%\n",
      "Step 283: loss = 0.016 metric = 99.57%\n",
      "Step 284: loss = 0.0158 metric = 99.59%\n",
      "Step 285: loss = 0.0156 metric = 99.59%\n",
      "Step 286: loss = 0.0154 metric = 99.59%\n",
      "Step 287: loss = 0.0152 metric = 99.59%\n",
      "Step 288: loss = 0.015 metric = 99.57%\n",
      "Step 289: loss = 0.0148 metric = 99.61%\n",
      "Step 290: loss = 0.0146 metric = 99.61%\n",
      "Step 291: loss = 0.0145 metric = 99.59%\n",
      "Step 292: loss = 0.0143 metric = 99.61%\n",
      "Step 293: loss = 0.0141 metric = 99.61%\n",
      "Step 294: loss = 0.0139 metric = 99.64%\n",
      "Step 295: loss = 0.0137 metric = 99.64%\n",
      "Step 296: loss = 0.0135 metric = 99.64%\n",
      "Step 297: loss = 0.0133 metric = 99.64%\n",
      "Step 298: loss = 0.013 metric = 99.64%\n",
      "Step 299: loss = 0.0128 metric = 99.66%\n",
      "Step 300: loss = 0.0125 metric = 99.64%\n",
      "Step 301: loss = 0.0124 metric = 99.66%\n",
      "Step 302: loss = 0.0123 metric = 99.66%\n",
      "Step 303: loss = 0.0121 metric = 99.66%\n",
      "Step 304: loss = 0.012 metric = 99.64%\n",
      "Step 305: loss = 0.0118 metric = 99.66%\n",
      "Step 306: loss = 0.0117 metric = 99.64%\n",
      "Step 307: loss = 0.0115 metric = 99.64%\n",
      "Step 308: loss = 0.0114 metric = 99.64%\n",
      "Step 309: loss = 0.0113 metric = 99.66%\n",
      "Step 310: loss = 0.0112 metric = 99.68%\n",
      "Step 311: loss = 0.0111 metric = 99.68%\n",
      "Step 312: loss = 0.0109 metric = 99.68%\n",
      "Step 313: loss = 0.0107 metric = 99.68%\n",
      "Step 314: loss = 0.0106 metric = 99.68%\n",
      "Step 315: loss = 0.0104 metric = 99.66%\n",
      "Step 316: loss = 0.0103 metric = 99.70%\n",
      "Step 317: loss = 0.01 metric = 99.66%\n",
      "Step 318: loss = 0.01 metric = 99.73%\n",
      "Step 319: loss = 0.0099 metric = 99.70%\n",
      "Step 320: loss = 0.0101 metric = 99.70%\n",
      "Step 321: loss = 0.00953 metric = 99.73%\n",
      "Step 322: loss = 0.00929 metric = 99.73%\n",
      "Step 323: loss = 0.00929 metric = 99.73%\n",
      "Step 324: loss = 0.00908 metric = 99.70%\n",
      "Step 325: loss = 0.00898 metric = 99.73%\n",
      "Step 326: loss = 0.00882 metric = 99.70%\n",
      "Step 327: loss = 0.00872 metric = 99.73%\n",
      "Step 328: loss = 0.00871 metric = 99.75%\n",
      "Step 329: loss = 0.00863 metric = 99.75%\n",
      "Step 330: loss = 0.00869 metric = 99.75%\n",
      "Step 331: loss = 0.00833 metric = 99.75%\n",
      "Step 332: loss = 0.00848 metric = 99.75%\n",
      "Step 333: loss = 0.00914 metric = 99.75%\n",
      "Step 334: loss = 0.00817 metric = 99.75%\n",
      "Step 335: loss = 0.0104 metric = 99.73%\n",
      "Step 336: loss = 0.201 metric = 94.38%\n",
      "Step 337: loss = 84.8 metric = 67.35%\n",
      "Step 338: loss = 3.31 metric = 47.47%\n",
      "Step 339: loss = 3.9 metric = 37.91%\n",
      "Step 340: loss = 4.39 metric = 28.96%\n",
      "Step 341: loss = 4.52 metric = 26.80%\n",
      "Step 342: loss = 4.21 metric = 27.96%\n",
      "Step 343: loss = 3.81 metric = 30.65%\n",
      "Step 344: loss = 3.44 metric = 33.01%\n",
      "Step 345: loss = 3.11 metric = 34.63%\n",
      "Step 346: loss = 2.8 metric = 36.45%\n",
      "Step 347: loss = 2.52 metric = 37.16%\n",
      "Step 348: loss = 2.26 metric = 37.77%\n",
      "Step 349: loss = 2.04 metric = 39.91%\n",
      "Step 350: loss = 1.85 metric = 41.92%\n",
      "Step 351: loss = 1.7 metric = 44.83%\n",
      "Step 352: loss = 1.58 metric = 46.17%\n",
      "Step 353: loss = 1.47 metric = 48.32%\n",
      "Step 354: loss = 1.38 metric = 50.14%\n",
      "Step 355: loss = 1.3 metric = 52.62%\n",
      "Step 356: loss = 1.23 metric = 55.15%\n",
      "Step 357: loss = 1.16 metric = 56.88%\n",
      "Step 358: loss = 1.11 metric = 58.29%\n",
      "Step 359: loss = 1.06 metric = 59.68%\n",
      "Step 360: loss = 1.02 metric = 61.09%\n",
      "Step 361: loss = 0.981 metric = 62.20%\n",
      "Step 362: loss = 0.949 metric = 62.91%\n",
      "Step 363: loss = 0.923 metric = 63.52%\n",
      "Step 364: loss = 0.899 metric = 64.28%\n",
      "Step 365: loss = 0.877 metric = 64.75%\n",
      "Step 366: loss = 0.857 metric = 65.19%\n",
      "Step 367: loss = 0.839 metric = 65.64%\n",
      "Step 368: loss = 0.822 metric = 66.12%\n",
      "Step 369: loss = 0.806 metric = 66.30%\n",
      "Step 370: loss = 0.792 metric = 66.83%\n",
      "Step 371: loss = 0.779 metric = 67.05%\n",
      "Step 372: loss = 0.767 metric = 67.33%\n",
      "Step 373: loss = 0.757 metric = 67.58%\n",
      "Step 374: loss = 0.747 metric = 68.01%\n",
      "Step 375: loss = 0.738 metric = 68.19%\n",
      "Step 376: loss = 0.731 metric = 68.40%\n",
      "Step 377: loss = 0.723 metric = 68.65%\n",
      "Step 378: loss = 0.717 metric = 68.90%\n",
      "Step 379: loss = 0.71 metric = 69.44%\n",
      "Step 380: loss = 0.704 metric = 69.69%\n",
      "Step 381: loss = 0.699 metric = 70.10%\n",
      "Step 382: loss = 0.694 metric = 70.38%\n",
      "Step 383: loss = 0.689 metric = 70.63%\n",
      "Step 384: loss = 0.684 metric = 70.90%\n",
      "Step 385: loss = 0.679 metric = 71.15%\n",
      "Step 386: loss = 0.675 metric = 71.38%\n",
      "Step 387: loss = 0.671 metric = 71.77%\n",
      "Step 388: loss = 0.667 metric = 71.99%\n",
      "Step 389: loss = 0.664 metric = 72.22%\n",
      "Step 390: loss = 0.66 metric = 72.38%\n",
      "Step 391: loss = 0.657 metric = 72.45%\n",
      "Step 392: loss = 0.653 metric = 72.45%\n",
      "Step 393: loss = 0.65 metric = 72.63%\n",
      "Step 394: loss = 0.647 metric = 72.75%\n",
      "Step 395: loss = 0.644 metric = 72.84%\n",
      "Step 396: loss = 0.641 metric = 72.86%\n",
      "Step 397: loss = 0.639 metric = 72.97%\n",
      "Step 398: loss = 0.636 metric = 73.02%\n",
      "Step 399: loss = 0.634 metric = 73.04%\n",
      "Step 400: loss = 0.631 metric = 73.11%\n",
      "Step 401: loss = 0.629 metric = 73.25%\n",
      "Step 402: loss = 0.627 metric = 73.36%\n",
      "Step 403: loss = 0.625 metric = 73.45%\n",
      "Step 404: loss = 0.624 metric = 73.50%\n",
      "Step 405: loss = 0.622 metric = 73.57%\n",
      "Step 406: loss = 0.62 metric = 73.59%\n",
      "Step 407: loss = 0.619 metric = 73.72%\n",
      "Step 408: loss = 0.617 metric = 73.27%\n",
      "Step 409: loss = 0.616 metric = 73.52%\n",
      "Step 410: loss = 0.614 metric = 73.57%\n",
      "Step 411: loss = 0.613 metric = 73.63%\n",
      "Step 412: loss = 0.611 metric = 73.50%\n",
      "Step 413: loss = 0.61 metric = 74.11%\n",
      "Step 414: loss = 0.608 metric = 74.13%\n",
      "Step 415: loss = 0.607 metric = 74.16%\n",
      "Step 416: loss = 0.605 metric = 74.27%\n",
      "Step 417: loss = 0.604 metric = 74.36%\n",
      "Step 418: loss = 0.602 metric = 74.45%\n",
      "Step 419: loss = 0.601 metric = 74.54%\n",
      "Step 420: loss = 0.6 metric = 74.66%\n",
      "Step 421: loss = 0.598 metric = 74.66%\n",
      "Step 422: loss = 0.597 metric = 74.66%\n",
      "interrupted by user\n"
     ]
    }
   ],
   "source": [
    "model = RNN(len(labels), tokenizer, 100, 100)\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0E-3)\n",
    "train = Batchifyer(df_train, tokenizer, n_batches=1, batch_size=None)\n",
    "val = Batchifyer(df_val, tokenizer, n_batches=1, batch_size=None)\n",
    "train_loop(model, optimizer, train, val, n_steps=1000, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 99.772%\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_to_tensor(df_test, tokenizer)\n",
    "y_pred = model.predict(X)\n",
    "acc = accuracy(y_pred, Y.to(y_pred.device))\n",
    "print(f\"accuracy {acc:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         neutral\n",
       "10        neutral\n",
       "13       positive\n",
       "19       positive\n",
       "20       negative\n",
       "           ...   \n",
       "14587    negative\n",
       "14588     neutral\n",
       "14593    negative\n",
       "14600     neutral\n",
       "14612    negative\n",
       "Name: airline_sentiment, Length: 2196, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.Series([labels[i] for i in y_pred.cpu().numpy()], index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred != df_test[\"airline_sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice II\n",
    "\n",
    "Programmer un modèle type encodeur de transformeur pour classifier les tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2017a09ab1f54c7a6a1af190715fd60264df4a93389a277e03c18d947a6e489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from typing import Callable, Iterable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialToken:\n",
    "    \"\"\"\n",
    "    A special token for tokenizers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, string: str):\n",
    "        self.string = string.upper()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.string}>\"\n",
    "    \n",
    "    def __eq__(self, other) -> bool:\n",
    "        if isinstance(other, SpecialToken):\n",
    "            return self.string == other.string\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.string)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    A simple word tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    word_pattern = re.compile(R\"\\w+|\\d+|[^\\w\\d\\s]\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tokenizer({len(self.vocabulary)} tokens)\"\n",
    "\n",
    "    def __init__(self, corpus: Iterable[str], min_frequency: float = 1.0E-6):\n",
    "        words = [word for document in corpus for word in self._split(document)]\n",
    "        word_counts = Counter(words)\n",
    "        word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.vocabulary = [k for k, v in word_counts if v/len(words) >= min_frequency] + [SpecialToken(\"UNKNOWN\"), SpecialToken(\"END\"), SpecialToken(\"PAD\")]\n",
    "        self.map = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def split(self, document: str) -> list[str]:\n",
    "        return self.decode(self.encode(document))\n",
    "\n",
    "    def encode(self, document: str) -> list[int]:\n",
    "        return [self.map.get(word, self.UNKNOWN) for word in self._split(document)]\n",
    "\n",
    "    def decode(self, encoded: list[int]) -> list[str]:\n",
    "        return [i.string if isinstance(i, SpecialToken) else self.vocabulary[i] for i in encoded]\n",
    "\n",
    "    def _split(self, document: str) -> list[str]:\n",
    "        return self.word_pattern.findall(document)\n",
    "\n",
    "    @property\n",
    "    def PAD(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"PAD\")]\n",
    "    \n",
    "    @property\n",
    "    def END(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"END\")]\n",
    "    \n",
    "    @property\n",
    "    def UNKNOWN(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"UNKNOWN\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'airline_sentiment'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/Twitter_US_Airline_Sentiment.csv\")\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(4314 tokens)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(df.text, min_frequency=1.0E-5)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USAirways @AmericanAir terminal E in Miami is still the worst most smelly airport ever. Thanks for nothing.\n",
      "['@', 'USAirways', '@', 'AmericanAir', 'terminal', 'E', 'in', 'Miami', 'is', 'still', 'the', 'worst', 'most', <UNKNOWN>, 'airport', 'ever', '.', 'Thanks', 'for', 'nothing', '.']\n",
      "\n",
      "@united Arriving 25 minutes early is nice, but not if equipment isn't ready. Waiting 30 minutes for luggage, so far. Time gains wiped out.\n",
      "['@', 'united', <UNKNOWN>, '25', 'minutes', 'early', 'is', 'nice', ',', 'but', 'not', 'if', 'equipment', 'isn', \"'\", 't', 'ready', '.', 'Waiting', '30', 'minutes', 'for', 'luggage', ',', 'so', 'far', '.', 'Time', <UNKNOWN>, <UNKNOWN>, 'out', '.']\n",
      "\n",
      "@SouthwestAir grouchy about this flight 636 #complimentarybeveragesneeded\n",
      "['@', 'SouthwestAir', <UNKNOWN>, 'about', 'this', 'flight', <UNKNOWN>, '#', <UNKNOWN>]\n",
      "\n",
      "@JetBlue 2 aisles of empty #evermoreroom seats and we can't move bc we didn't pay?! #nonsense #Waste #JetBlue #jetbluebos #cheap\n",
      "['@', 'JetBlue', '2', <UNKNOWN>, 'of', 'empty', '#', <UNKNOWN>, 'seats', 'and', 'we', 'can', \"'\", 't', 'move', 'bc', 'we', 'didn', \"'\", 't', 'pay', '?', '!', '#', 'nonsense', '#', <UNKNOWN>, '#', 'JetBlue', '#', <UNKNOWN>, '#', 'cheap']\n",
      "\n",
      "@united my flight landed 50 min, but we are being told to stay on the plane, haven't pulled up to the gate &amp; have little to no information.\n",
      "['@', 'united', 'my', 'flight', 'landed', '50', 'min', ',', 'but', 'we', 'are', 'being', 'told', 'to', 'stay', 'on', 'the', 'plane', ',', 'haven', \"'\", 't', 'pulled', 'up', 'to', 'the', 'gate', '&', 'amp', ';', 'have', 'little', 'to', 'no', 'information', '.']\n",
      "\n",
      "@AmericanAir you really need some customer service training for your unhappy EEs in the morning in Chicago. Gate K20 at 430 chking her schd\n",
      "['@', 'AmericanAir', 'you', 'really', 'need', 'some', 'customer', 'service', 'training', 'for', 'your', 'unhappy', <UNKNOWN>, 'in', 'the', 'morning', 'in', 'Chicago', '.', 'Gate', <UNKNOWN>, 'at', <UNKNOWN>, <UNKNOWN>, 'her', <UNKNOWN>]\n",
      "\n",
      "Peak #Caucasity RT @JetBlue Our fleet's on fleek. http://t.co/cSNRPVY9b9\n",
      "[<UNKNOWN>, '#', <UNKNOWN>, 'RT', '@', 'JetBlue', 'Our', 'fleet', \"'\", 's', 'on', 'fleek', '.', 'http', ':', '/', '/', 't', '.', 'co', '/', <UNKNOWN>]\n",
      "\n",
      "@united thanks again for your concern. I will contact customer care upon our return from Australia.\n",
      "['@', 'united', 'thanks', 'again', 'for', 'your', 'concern', '.', 'I', 'will', 'contact', 'customer', 'care', 'upon', 'our', 'return', 'from', 'Australia', '.']\n",
      "\n",
      "@united Aw, thanks for the kind words. Totally makes these extra 6 hours sitting in an airport SOOOO much better.\n",
      "['@', 'united', <UNKNOWN>, ',', 'thanks', 'for', 'the', 'kind', 'words', '.', 'Totally', 'makes', 'these', 'extra', '6', 'hours', 'sitting', 'in', 'an', 'airport', <UNKNOWN>, 'much', 'better', '.']\n",
      "\n",
      ".@united Thanks. Hopefully this is easily resolved.\n",
      "['.', '@', 'united', 'Thanks', '.', 'Hopefully', 'this', 'is', 'easily', 'resolved', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in df.sample(n=10).text:\n",
    "    print(text)\n",
    "    print(tokenizer.split(text))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7)\n",
    "df = df.drop(index=df_train.index)\n",
    "df_val = df.sample(frac=0.5)\n",
    "df_test = df.drop(index=df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted: torch.Tensor, target: torch.Tensor):\n",
    "    assert target.shape == predicted.shape\n",
    "    assert target.dtype == torch.long\n",
    "    assert predicted.dtype == torch.long\n",
    "    with torch.no_grad():\n",
    "        return torch.mean((predicted == target).float()).cpu().item()\n",
    "\n",
    "\n",
    "def input_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> torch.Tensor:\n",
    "    encoded = [tokenizer.encode(document) for document in df['text']]\n",
    "    L = max([len(doc) for doc in encoded])\n",
    "    encoded = [doc + [tokenizer.PAD]*(L - len(doc)) for doc in encoded]\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "def target_to_tensor(df: pd.DataFrame) -> torch.Tensor:\n",
    "    map = {k: v for v, k in enumerate(labels)}\n",
    "    return torch.tensor([map[label] for label in df[\"airline_sentiment\"]], dtype=torch.long)\n",
    "\n",
    "\n",
    "def data_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> tuple[torch.Tensor]:\n",
    "    return (input_to_tensor(df, tokenizer), target_to_tensor(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batchifyer:\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: Tokenizer, n_batches: int, batch_size: Optional[int]):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_batches = n_batches\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        shuffled = df.sample(frac=1.)\n",
    "        return (self._batch(shuffled, i) for i in range(self.n_batches))\n",
    "    \n",
    "    def _batch(self, shuffled: pd.DataFrame, i: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = self.batch_size or len(shuffled) // self.n_batches\n",
    "        subset = shuffled.iloc[i*batch_size:(i+1)*batch_size]\n",
    "        return data_to_tensor(subset, self.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_data: Iterable[tuple[torch.Tensor]], val_data: Iterable[tuple[torch.Tensor]], n_steps: int = 1000, patience: int = 100, keep_best: bool = True):\n",
    "    \"\"\"\n",
    "    train the model for the specified number of steps, or untilearly stopping\n",
    "    \"\"\"\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_step = 0\n",
    "    best_metric = 0.\n",
    "    try:\n",
    "        for step in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "            # train loss\n",
    "            model.train()\n",
    "            losses = []\n",
    "            for x, y in train_data:\n",
    "                loss = model.loss(x, y)\n",
    "                loss.backward()\n",
    "                losses.append(loss.item())\n",
    "            loss = sum(losses)/len(losses)\n",
    "            # val metric\n",
    "            model.eval()\n",
    "            metrics = []\n",
    "            for x, y in val_data:\n",
    "                metrics.append(model.metric(x, y))\n",
    "            metric = sum(metrics) / len(metrics)\n",
    "            # checkpointing\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_step = step\n",
    "                if keep_best:\n",
    "                    best_state = deepcopy(model.state_dict())\n",
    "            elif step - best_step > patience:\n",
    "                print(\"early stoping\")\n",
    "                break\n",
    "            # optimizer steping\n",
    "            optimizer.step()\n",
    "            # printing\n",
    "            print(f\"Step {step}: loss = {loss:.3g} metric = {metric:.2%}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted by user\")\n",
    "    if keep_best:\n",
    "        model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "\n",
    "Implémenter et entraîner un réseau récurrent pour classifier les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes: int, tokenizer: Tokenizer, in_features: int, hidden_state_features: int, activation: Callable = torch.relu):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_state_features = hidden_state_features\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.vocabulary), in_features)\n",
    "        self.linear = torch.nn.Linear(in_features + hidden_state_features, in_features + hidden_state_features)\n",
    "        self.activation = activation\n",
    "        self.contract = torch.nn.Linear(in_features + hidden_state_features, hidden_state_features)\n",
    "        self.normalization = torch.nn.LayerNorm(hidden_state_features)\n",
    "        self.output = torch.nn.Linear(hidden_state_features, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : torch.Tensor\n",
    "            tensor of long of shape (N, L)\n",
    "        \"\"\"\n",
    "        X = X.to(self.device)\n",
    "        N, L = X.shape\n",
    "        H = torch.zeros((N, self.hidden_state_features), dtype=torch.float32, device=X.device)\n",
    "        for x in X.transpose(0, 1):\n",
    "            I = self.embedding(x)\n",
    "            T = torch.cat([I, H], dim=1)\n",
    "            T = self.linear(T)\n",
    "            T = self.activation(T)\n",
    "            T = self.contract(T)\n",
    "            H = torch.where(x.unsqueeze(1) == self.tokenizer.PAD, H, T)\n",
    "        return self.output(H)\n",
    "    \n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Y = self(X)\n",
    "        return Y.max(dim=1).indices\n",
    "    \n",
    "    def loss(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self(X)\n",
    "        return F.cross_entropy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    def metric(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.03 metric = 62.61%\n",
      "Step 1: loss = 0.969 metric = 63.09%\n",
      "Step 2: loss = 0.921 metric = 63.09%\n",
      "Step 3: loss = 0.893 metric = 63.09%\n",
      "Step 4: loss = 0.886 metric = 63.09%\n",
      "Step 5: loss = 0.889 metric = 63.09%\n",
      "Step 6: loss = 0.886 metric = 63.09%\n",
      "Step 7: loss = 0.877 metric = 63.09%\n",
      "interrupted by user\n"
     ]
    }
   ],
   "source": [
    "model = RNN(len(labels), tokenizer, 100, 100)\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0E-3)\n",
    "train = Batchifyer(df_train, tokenizer, n_batches=1, batch_size=None)\n",
    "val = Batchifyer(df_val, tokenizer, n_batches=1, batch_size=None)\n",
    "train_loop(model, optimizer, train, val, n_steps=1000, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 61.840%\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_to_tensor(df_test, tokenizer)\n",
    "y_pred = model.predict(X)\n",
    "acc = accuracy(y_pred, Y.to(y_pred.device))\n",
    "print(f\"accuracy {acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice II\n",
    "\n",
    "Programmer un modèle type encodeur de transformeur pour classifier les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, projection_dim: int, n_heads: int, activation: Callable):\n",
    "        super().__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.n_heads = n_heads\n",
    "        D = projection_dim * n_heads\n",
    "        self.q = torch.nn.Linear(D, D, bias=False)\n",
    "        self.k = torch.nn.Linear(D, D, bias=False)\n",
    "        self.v = torch.nn.Linear(D, D, bias=False)\n",
    "        self.intermediate_norm = torch.nn.LayerNorm(D)\n",
    "        self.expand = torch.nn.Linear(D, 4*D)\n",
    "        self.activation = activation\n",
    "        self.contract = torch.nn.LayerNorm(4*D, D)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor, mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : torch.Tensor\n",
    "            tensor of shape (N, L, D)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        input = X\n",
    "        N, L, D = X.shape\n",
    "        X = X.reshape(-1, D)\n",
    "        Q = self.q(X).reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        K = self.k(X).reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        V = self.v(X).reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        S = torch.einsum(\"nhld, nhkd -> nhlk\", Q, K)\n",
    "        S = torch.masked_fill(S, mask.reshape(1, 1, L, L), -float(\"inf\"))\n",
    "        X = (S @ V).permute(0, 2, 1, 3).reshape(N, L, D)\n",
    "        X = self.intermediate_norm((X + input).reshape(-1, D)).reshape(N, L, D)\n",
    "        intermediate = X\n",
    "        X = self.expand(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.contract(X)\n",
    "        X = self.norm((X + intermediate).reshape(-1, D)).reshape(N, L, D)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes: int, tokenizer: Tokenizer, n_stages: int, projection_dim: int, n_heads: int, activation: Callable = torch.relu):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.vocabulary), n_heads*projection_dim)\n",
    "        self.stages = torch.nn.ModuleList()\n",
    "        for _ in range(n_stages):\n",
    "            self.stages.append(AttentionBlock(projection_dim, n_heads, activation))\n",
    "        self.output = torch.nn.Linear(projection_dim * n_heads, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X.to(self.device)\n",
    "        mask = (X == self.tokenizer.PAD)\n",
    "        mask = (mask.unsqueeze(1) | mask.unsqueeze(0))\n",
    "        X = self.embedding(X)\n",
    "        for stage in self.stages:\n",
    "            X = stage(X, mask)\n",
    "            # if self.training:\n",
    "            #     print(stage.q)\n",
    "            #     X = checkpoint(stage, X, mask)\n",
    "            # else:\n",
    "            #     X = stage(X, mask)\n",
    "        return self.output(X)\n",
    "    \n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Y = self(X)\n",
    "        return Y.max(dim=1).indices\n",
    "    \n",
    "    def loss(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self(X)\n",
    "        return F.cross_entropy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    def metric(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AttentionBlock' object has no attribute 'q'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Benoit\\Documents\\projets\\bfavier.github.io\\introduction_to_deep_learning\\notebooks\\introduction_to_deep_learning_TD_3.ipynb Cellule 17\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train \u001b[39m=\u001b[39m Batchifyer(df_train, tokenizer, n_batches\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m val \u001b[39m=\u001b[39m Batchifyer(df_val, tokenizer, n_batches\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_loop(model, optimizer, train, val, n_steps\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Benoit\\Documents\\projets\\bfavier.github.io\\introduction_to_deep_learning\\notebooks\\introduction_to_deep_learning_TD_3.ipynb Cellule 17\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, optimizer, train_data, val_data, n_steps, patience, keep_best)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_data:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloss(x, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;32mc:\\Users\\Benoit\\Documents\\projets\\bfavier.github.io\\introduction_to_deep_learning\\notebooks\\introduction_to_deep_learning_TD_3.ipynb Cellule 17\u001b[0m in \u001b[0;36mTransformer.loss\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, X: torch\u001b[39m.\u001b[39mTensor, Y: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mcross_entropy(y_pred, Y\u001b[39m.\u001b[39mto(y_pred\u001b[39m.\u001b[39mdevice))\n",
      "File \u001b[1;32mc:\\Users\\Benoit\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Benoit\\Documents\\projets\\bfavier.github.io\\introduction_to_deep_learning\\notebooks\\introduction_to_deep_learning_TD_3.ipynb Cellule 17\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m stage \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstages:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     X \u001b[39m=\u001b[39m stage(X, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# if self.training:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m#     print(stage.q)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m#     X = checkpoint(stage, X, mask)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#     X = stage(X, mask)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(X)\n",
      "File \u001b[1;32mc:\\Users\\Benoit\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Benoit\\Documents\\projets\\bfavier.github.io\\introduction_to_deep_learning\\notebooks\\introduction_to_deep_learning_TD_3.ipynb Cellule 17\u001b[0m in \u001b[0;36mAttentionBlock.forward\u001b[1;34m(self, X, mask)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m N, L, D \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, D)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m Q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq(X)\u001b[39m.\u001b[39mreshape(N, L, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_dim)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m K \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk(X)\u001b[39m.\u001b[39mreshape(N, L, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_dim)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benoit/Documents/projets/bfavier.github.io/introduction_to_deep_learning/notebooks/introduction_to_deep_learning_TD_3.ipynb#X40sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m V \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv(X)\u001b[39m.\u001b[39mreshape(N, L, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_dim)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Benoit\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AttentionBlock' object has no attribute 'q'"
     ]
    }
   ],
   "source": [
    "model = Transformer(len(labels), tokenizer, 4, 16, 8)\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0E-3)\n",
    "train = Batchifyer(df_train, tokenizer, n_batches=1, batch_size=None)\n",
    "val = Batchifyer(df_val, tokenizer, n_batches=1, batch_size=None)\n",
    "train_loop(model, optimizer, train, val, n_steps=1000, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2017a09ab1f54c7a6a1af190715fd60264df4a93389a277e03c18d947a6e489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

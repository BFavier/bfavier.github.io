{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benoit\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from typing import Callable, Iterable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialToken:\n",
    "    \"\"\"\n",
    "    A special token for tokenizers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, string: str):\n",
    "        self.string = string.upper()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.string}>\"\n",
    "    \n",
    "    def __eq__(self, other) -> bool:\n",
    "        if isinstance(other, SpecialToken):\n",
    "            return self.string == other.string\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.string)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    A simple word tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    word_pattern = re.compile(R\"\\w+|\\d+|[^\\w\\d\\s]\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tokenizer({len(self.vocabulary)} tokens)\"\n",
    "\n",
    "    def __init__(self, corpus: Iterable[str], min_frequency: float = 1.0E-6):\n",
    "        words = [word for document in corpus for word in self._split(document)]\n",
    "        word_counts = Counter(words)\n",
    "        word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.vocabulary = [k for k, v in word_counts if v/len(words) >= min_frequency] + [SpecialToken(\"UNKNOWN\"), SpecialToken(\"END\"), SpecialToken(\"PAD\")]\n",
    "        self.map = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def split(self, document: str) -> list[str]:\n",
    "        return self.decode(self.encode(document))\n",
    "\n",
    "    def encode(self, document: str) -> list[int]:\n",
    "        return [self.map.get(word, self.UNKNOWN) for word in self._split(document)]\n",
    "\n",
    "    def decode(self, encoded: list[int]) -> list[str]:\n",
    "        return [i.string if isinstance(i, SpecialToken) else self.vocabulary[i] for i in encoded]\n",
    "\n",
    "    def _split(self, document: str) -> list[str]:\n",
    "        return self.word_pattern.findall(document)\n",
    "\n",
    "    @property\n",
    "    def PAD(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"PAD\")]\n",
    "    \n",
    "    @property\n",
    "    def END(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"END\")]\n",
    "    \n",
    "    @property\n",
    "    def UNKNOWN(self) -> SpecialToken:\n",
    "        return self.map[SpecialToken(\"UNKNOWN\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'airline_sentiment'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/Twitter_US_Airline_Sentiment.csv\")\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(4314 tokens)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(df.text, min_frequency=1.0E-5)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@united Will the airline refuse my Canadian passport with less than 6 months remaining validity? Going to US.\n",
      "['@', 'united', 'Will', 'the', 'airline', 'refuse', 'my', 'Canadian', 'passport', 'with', 'less', 'than', '6', 'months', <UNKNOWN>, <UNKNOWN>, '?', 'Going', 'to', 'US', '.']\n",
      "\n",
      "@JetBlue marks 15th birthday with @Airbus #A320 painted in 'Blumanity' paint job http://t.co/9vTFM7kNad\n",
      "['@', 'JetBlue', 'marks', '15th', 'birthday', 'with', '@', 'Airbus', '#', 'A320', <UNKNOWN>, 'in', \"'\", <UNKNOWN>, \"'\", <UNKNOWN>, 'job', 'http', ':', '/', '/', 't', '.', 'co', '/', <UNKNOWN>]\n",
      "\n",
      "@JetBlue thank you. Appreciate that!!\n",
      "['@', 'JetBlue', 'thank', 'you', '.', 'Appreciate', 'that', '!', '!']\n",
      "\n",
      "@USAirways I saw online you had fee but I was wondering if there was a way to pay for it ahead of time, instead of at the kiosk\n",
      "['@', 'USAirways', 'I', 'saw', 'online', 'you', 'had', 'fee', 'but', 'I', 'was', 'wondering', 'if', 'there', 'was', 'a', 'way', 'to', 'pay', 'for', 'it', 'ahead', 'of', 'time', ',', 'instead', 'of', 'at', 'the', 'kiosk']\n",
      "\n",
      "@USAirways Absolutely!! The staff was amazing!!\n",
      "['@', 'USAirways', <UNKNOWN>, '!', '!', 'The', 'staff', 'was', 'amazing', '!', '!']\n",
      "\n",
      "@JetBlue's new CEO seeks the right balance to please passengers and Wall ... - Greenfield Daily Reporter http://t.co/LM3opxkxch\n",
      "['@', 'JetBlue', \"'\", 's', 'new', 'CEO', 'seeks', 'the', 'right', 'balance', 'to', 'please', 'passengers', 'and', 'Wall', '.', '.', '.', '-', <UNKNOWN>, 'Daily', <UNKNOWN>, 'http', ':', '/', '/', 't', '.', 'co', '/', <UNKNOWN>]\n",
      "\n",
      "@USAirways Your policy needs a serious revision for babies.  Fortunately I have choices and choose @Southwest from here on out.\n",
      "['@', 'USAirways', 'Your', 'policy', 'needs', 'a', 'serious', <UNKNOWN>, 'for', <UNKNOWN>, '.', <UNKNOWN>, 'I', 'have', 'choices', 'and', 'choose', '@', 'Southwest', 'from', 'here', 'on', 'out', '.']\n",
      "\n",
      "@JetBlue not cool to announce a flight \"delay\" when everyone is checking in. A 4 hour delay bdl-dca with a staff offering  no help.\n",
      "['@', 'JetBlue', 'not', 'cool', 'to', 'announce', 'a', 'flight', '\"', 'delay', '\"', 'when', 'everyone', 'is', 'checking', 'in', '.', 'A', '4', 'hour', 'delay', <UNKNOWN>, '-', 'dca', 'with', 'a', 'staff', 'offering', 'no', 'help', '.']\n",
      "\n",
      "@united deceptive marketing practices promised me if i booked flights i would retain my status now they are not granting. Neverflyunited\n",
      "['@', 'united', <UNKNOWN>, 'marketing', <UNKNOWN>, 'promised', 'me', 'if', 'i', 'booked', 'flights', 'i', 'would', <UNKNOWN>, 'my', 'status', 'now', 'they', 'are', 'not', <UNKNOWN>, '.', <UNKNOWN>]\n",
      "\n",
      "@united sorry, wrong link for the bag : http://t.co/pZAl4wtrEZ Thats the one i meant\n",
      "['@', 'united', 'sorry', ',', 'wrong', 'link', 'for', 'the', 'bag', ':', 'http', ':', '/', '/', 't', '.', 'co', '/', <UNKNOWN>, <UNKNOWN>, 'the', 'one', 'i', 'meant']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in df.sample(n=10).iterrows():\n",
    "    print(text)\n",
    "    print(tokenizer.split(text))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7)\n",
    "df = df.drop(index=df_train.index)\n",
    "df_val = df.sample(frac=0.5)\n",
    "df_test = df.drop(index=df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted: torch.Tensor, target: torch.Tensor):\n",
    "    assert target.shape == predicted.shape\n",
    "    assert target.dtype == torch.long\n",
    "    assert predicted.dtype == torch.long\n",
    "    with torch.no_grad():\n",
    "        return torch.mean((predicted == target).float()).cpu().item()\n",
    "\n",
    "\n",
    "def input_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> torch.Tensor:\n",
    "    encoded = [tokenizer.encode(document) for document in df['text']]\n",
    "    L = max([len(doc) for doc in encoded])\n",
    "    encoded = [doc + [tokenizer.PAD]*(L - len(doc)) for doc in encoded]\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "def target_to_tensor(df: pd.DataFrame) -> torch.Tensor:\n",
    "    map = {k: v for v, k in enumerate(labels)}\n",
    "    return torch.tensor([map[label] for label in df[\"airline_sentiment\"]], dtype=torch.long)\n",
    "\n",
    "\n",
    "def data_to_tensor(df: pd.DataFrame, tokenizer: Tokenizer) -> tuple[torch.Tensor]:\n",
    "    return (input_to_tensor(df, tokenizer), target_to_tensor(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batchifyer:\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: Tokenizer, n_batches: int, batch_size: Optional[int]):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_batches = n_batches\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        shuffled = df.sample(frac=1.)\n",
    "        return (self._batch(shuffled, i) for i in range(self.n_batches))\n",
    "    \n",
    "    def _batch(self, shuffled: pd.DataFrame, i: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = self.batch_size or len(shuffled) // self.n_batches\n",
    "        subset = shuffled.iloc[i*batch_size:(i+1)*batch_size]\n",
    "        return data_to_tensor(subset, self.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_data: Iterable[tuple[torch.Tensor]], val_data: Iterable[tuple[torch.Tensor]], n_steps: int = 1000, patience: int = 100, keep_best: bool = True):\n",
    "    \"\"\"\n",
    "    train the model for the specified number of steps, or untilearly stopping\n",
    "    \"\"\"\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_step = 0\n",
    "    best_metric = 0.\n",
    "    try:\n",
    "        for step in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "            # train loss\n",
    "            model.train()\n",
    "            losses = []\n",
    "            for x, y in train_data:\n",
    "                loss = model.loss(x, y)\n",
    "                loss.backward()\n",
    "                losses.append(loss.item())\n",
    "            loss = sum(losses)/len(losses)\n",
    "            # val metric\n",
    "            model.eval()\n",
    "            metrics = []\n",
    "            for x, y in val_data:\n",
    "                metrics.append(model.metric(x, y))\n",
    "            metric = sum(metrics) / len(metrics)\n",
    "            # checkpointing\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_step = step\n",
    "                if keep_best:\n",
    "                    best_state = deepcopy(model.state_dict())\n",
    "            elif step - best_step > patience:\n",
    "                print(\"early stoping\")\n",
    "                break\n",
    "            # optimizer steping\n",
    "            optimizer.step()\n",
    "            # printing\n",
    "            print(f\"Step {step}: loss = {loss:.3g} metric = {metric:.2%}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted by user\")\n",
    "    if keep_best:\n",
    "        model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "\n",
    "Implémenter et entraîner un réseau récurrent pour classifier les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes: int, tokenizer: Tokenizer, in_features: int, hidden_state_features: int, activation: Callable = torch.relu):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_state_features = hidden_state_features\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.vocabulary), in_features)\n",
    "        self.linear = torch.nn.Linear(in_features + hidden_state_features, in_features + hidden_state_features)\n",
    "        self.activation = activation\n",
    "        self.contract = torch.nn.Linear(in_features + hidden_state_features, hidden_state_features)\n",
    "        self.normalization = torch.nn.LayerNorm(hidden_state_features)\n",
    "        self.output = torch.nn.Linear(hidden_state_features, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : torch.Tensor\n",
    "            tensor of long of shape (N, L)\n",
    "        \"\"\"\n",
    "        X = X.to(self.device)\n",
    "        N, L = X.shape\n",
    "        H = torch.zeros((N, self.hidden_state_features), dtype=torch.float32, device=X.device)\n",
    "        for x in X.transpose(0, 1):\n",
    "            I = self.embedding(x)\n",
    "            T = torch.cat([I, H], dim=1)\n",
    "            T = self.linear(T)\n",
    "            T = self.activation(T)\n",
    "            T = self.contract(T)\n",
    "            H = torch.where(x.unsqueeze(1) == self.tokenizer.PAD, H, T)\n",
    "        return self.output(H)\n",
    "    \n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Y = self(X)\n",
    "        return Y.max(dim=1).indices\n",
    "    \n",
    "    def loss(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self(X)\n",
    "        return F.cross_entropy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    def metric(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.15 metric = 21.77%\n",
      "Step 1: loss = 1.06 metric = 58.61%\n",
      "Step 2: loss = 0.99 metric = 62.18%\n",
      "Step 3: loss = 0.939 metric = 62.23%\n",
      "Step 4: loss = 0.911 metric = 62.23%\n",
      "Step 5: loss = 0.905 metric = 62.23%\n",
      "Step 6: loss = 0.907 metric = 62.23%\n",
      "Step 7: loss = 0.903 metric = 62.23%\n",
      "Step 8: loss = 0.892 metric = 62.23%\n",
      "Step 9: loss = 0.88 metric = 62.23%\n",
      "Step 10: loss = 0.871 metric = 62.23%\n",
      "interrupted by user\n"
     ]
    }
   ],
   "source": [
    "model = RNN(len(labels), tokenizer, 100, 100)\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0E-3)\n",
    "train = Batchifyer(df_train, tokenizer, n_batches=1, batch_size=None)\n",
    "val = Batchifyer(df_val, tokenizer, n_batches=1, batch_size=None)\n",
    "train_loop(model, optimizer, train, val, n_steps=1000, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 61.794%\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_to_tensor(df_test, tokenizer)\n",
    "y_pred = model.predict(X)\n",
    "acc = accuracy(y_pred, Y.to(y_pred.device))\n",
    "print(f\"accuracy {acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice II\n",
    "\n",
    "Programmer un modèle type encodeur de transformeur pour classifier les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, projection_dim: int, n_heads: int, activation: Callable):\n",
    "        super().__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.n_heads = n_heads\n",
    "        D = projection_dim * n_heads\n",
    "        self.q = torch.nn.Linear(D, D, bias=False)\n",
    "        self.k = torch.nn.Linear(D, D, bias=False)\n",
    "        self.v = torch.nn.Linear(D, D, bias=False)\n",
    "        self.intermediate_norm = torch.nn.LayerNorm(D)\n",
    "        self.expand = torch.nn.Linear(D, 4*D)\n",
    "        self.activation = activation\n",
    "        self.contract = torch.nn.Linear(4*D, D)\n",
    "        self.out_norm = torch.nn.LayerNorm(D)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor, mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            tensor of shape (N, L, D)\n",
    "        mask : torch.Tensor\n",
    "            tensor of boooleans of shape (N, L, L)\n",
    "        \"\"\"\n",
    "        input = X\n",
    "        N, L, D = X.shape\n",
    "        X = X.reshape(-1, D)\n",
    "        Q = self.q(X)\n",
    "        Q = Q.reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        K = self.k(X).reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        V = self.v(X).reshape(N, L, self.n_heads, self.projection_dim).permute(0, 2, 1, 3)\n",
    "        S = torch.einsum(\"nhld, nhkd -> nhlk\", Q, K)\n",
    "        S = torch.masked_fill(S, mask.unsqueeze(1), -float(\"inf\"))\n",
    "        S = torch.softmax(S, dim=-1)\n",
    "        S = torch.masked_fill(S, mask.unsqueeze(1), 0)\n",
    "        X = (S @ V).permute(0, 2, 1, 3).reshape(N, L, D)\n",
    "        X = self.intermediate_norm((X + input).reshape(-1, D)).reshape(N, L, D)\n",
    "        intermediate = X\n",
    "        X = self.expand(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.contract(X)\n",
    "        X = self.out_norm((X + intermediate).reshape(-1, D)).reshape(N, L, D)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes: int, tokenizer: Tokenizer, n_stages: int, projection_dim: int, n_heads: int, activation: Callable = torch.relu):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.vocabulary), n_heads*projection_dim)\n",
    "        self.stages = torch.nn.ModuleList()\n",
    "        for _ in range(n_stages):\n",
    "            self.stages.append(AttentionBlock(projection_dim, n_heads, activation))\n",
    "        self.output = torch.nn.Linear(projection_dim * n_heads, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            tensor of shape (N, L)\n",
    "        \"\"\"\n",
    "        X = X.to(self.device)\n",
    "        mask = (X == self.tokenizer.PAD)\n",
    "        mask = (mask.unsqueeze(1) | mask.unsqueeze(2))\n",
    "        X = self.embedding(X)\n",
    "        for stage in self.stages:\n",
    "            if self.training:\n",
    "                X = checkpoint(stage, X, mask)\n",
    "            else:\n",
    "                X = stage(X, mask)\n",
    "        return self.output(X.mean(dim=1))\n",
    "    \n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Y = self(X)\n",
    "        return Y.max(dim=1).indices\n",
    "    \n",
    "    def loss(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self(X)\n",
    "        return F.cross_entropy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    def metric(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy(y_pred, Y.to(y_pred.device))\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.13 metric = 21.43%\n",
      "Step 1: loss = 1.3 metric = 62.23%\n",
      "Step 2: loss = 1.2 metric = 62.23%\n",
      "Step 3: loss = 0.98 metric = 62.23%\n",
      "Step 4: loss = 0.906 metric = 62.23%\n",
      "Step 5: loss = 0.962 metric = 62.98%\n",
      "Step 6: loss = 0.953 metric = 64.48%\n",
      "Step 7: loss = 0.898 metric = 64.05%\n",
      "Step 8: loss = 0.85 metric = 62.52%\n",
      "Step 9: loss = 0.832 metric = 62.25%\n",
      "Step 10: loss = 0.829 metric = 62.25%\n",
      "Step 11: loss = 0.822 metric = 62.27%\n",
      "Step 12: loss = 0.805 metric = 62.64%\n",
      "Step 13: loss = 0.782 metric = 64.39%\n",
      "Step 14: loss = 0.764 metric = 65.96%\n",
      "Step 15: loss = 0.759 metric = 68.74%\n",
      "Step 16: loss = 0.76 metric = 68.74%\n",
      "Step 17: loss = 0.751 metric = 69.03%\n",
      "Step 18: loss = 0.731 metric = 70.15%\n",
      "Step 19: loss = 0.714 metric = 70.97%\n",
      "Step 20: loss = 0.704 metric = 70.51%\n",
      "Step 21: loss = 0.7 metric = 70.90%\n",
      "Step 22: loss = 0.69 metric = 71.47%\n",
      "Step 23: loss = 0.676 metric = 72.52%\n",
      "Step 24: loss = 0.66 metric = 73.75%\n",
      "Step 25: loss = 0.651 metric = 73.66%\n",
      "Step 26: loss = 0.644 metric = 73.70%\n",
      "Step 27: loss = 0.632 metric = 74.29%\n",
      "Step 28: loss = 0.616 metric = 75.32%\n",
      "Step 29: loss = 0.601 metric = 76.12%\n",
      "Step 30: loss = 0.59 metric = 76.57%\n",
      "Step 31: loss = 0.578 metric = 77.25%\n",
      "Step 32: loss = 0.562 metric = 77.76%\n",
      "Step 33: loss = 0.545 metric = 78.64%\n",
      "Step 34: loss = 0.532 metric = 79.51%\n",
      "Step 35: loss = 0.525 metric = 80.03%\n",
      "Step 36: loss = 0.559 metric = 78.62%\n",
      "Step 37: loss = 0.516 metric = 79.64%\n",
      "Step 38: loss = 0.479 metric = 81.31%\n",
      "Step 39: loss = 0.485 metric = 80.78%\n",
      "Step 40: loss = 0.454 metric = 82.60%\n",
      "Step 41: loss = 0.456 metric = 82.42%\n",
      "Step 42: loss = 0.439 metric = 84.36%\n",
      "Step 43: loss = 0.423 metric = 85.04%\n",
      "Step 44: loss = 0.413 metric = 84.77%\n",
      "Step 45: loss = 0.395 metric = 85.34%\n",
      "Step 46: loss = 0.387 metric = 86.25%\n",
      "Step 47: loss = 0.368 metric = 87.39%\n",
      "Step 48: loss = 0.364 metric = 87.50%\n",
      "Step 49: loss = 0.342 metric = 88.41%\n",
      "Step 50: loss = 0.333 metric = 88.73%\n",
      "Step 51: loss = 0.319 metric = 88.91%\n",
      "Step 52: loss = 0.305 metric = 89.71%\n",
      "Step 53: loss = 0.294 metric = 90.46%\n",
      "Step 54: loss = 0.277 metric = 91.17%\n",
      "Step 55: loss = 0.266 metric = 91.37%\n",
      "Step 56: loss = 0.253 metric = 92.62%\n",
      "Step 57: loss = 0.236 metric = 93.40%\n",
      "Step 58: loss = 0.225 metric = 93.81%\n",
      "Step 59: loss = 0.215 metric = 94.42%\n",
      "Step 60: loss = 0.201 metric = 94.85%\n",
      "Step 61: loss = 0.184 metric = 95.95%\n",
      "Step 62: loss = 0.169 metric = 96.43%\n",
      "Step 63: loss = 0.159 metric = 96.79%\n",
      "Step 64: loss = 0.148 metric = 97.22%\n",
      "Step 65: loss = 0.137 metric = 97.70%\n",
      "Step 66: loss = 0.126 metric = 98.00%\n",
      "Step 67: loss = 0.118 metric = 98.13%\n",
      "Step 68: loss = 0.111 metric = 98.41%\n",
      "Step 69: loss = 0.104 metric = 98.68%\n",
      "Step 70: loss = 0.0974 metric = 98.82%\n",
      "Step 71: loss = 0.0928 metric = 98.98%\n",
      "Step 72: loss = 0.0885 metric = 99.07%\n",
      "Step 73: loss = 0.0842 metric = 99.13%\n",
      "Step 74: loss = 0.0808 metric = 99.23%\n",
      "Step 75: loss = 0.078 metric = 99.27%\n",
      "Step 76: loss = 0.0757 metric = 99.36%\n",
      "Step 77: loss = 0.0734 metric = 99.36%\n",
      "Step 78: loss = 0.0714 metric = 99.36%\n",
      "Step 79: loss = 0.0694 metric = 99.39%\n",
      "Step 80: loss = 0.0676 metric = 99.43%\n",
      "Step 81: loss = 0.0657 metric = 99.50%\n",
      "Step 82: loss = 0.0644 metric = 99.52%\n",
      "Step 83: loss = 0.0631 metric = 99.52%\n",
      "Step 84: loss = 0.0619 metric = 99.52%\n",
      "Step 85: loss = 0.0607 metric = 99.54%\n",
      "Step 86: loss = 0.0597 metric = 99.54%\n",
      "Step 87: loss = 0.0586 metric = 99.54%\n",
      "Step 88: loss = 0.0576 metric = 99.57%\n",
      "Step 89: loss = 0.0567 metric = 99.57%\n",
      "Step 90: loss = 0.0559 metric = 99.57%\n",
      "Step 91: loss = 0.055 metric = 99.57%\n",
      "Step 92: loss = 0.0542 metric = 99.57%\n",
      "Step 93: loss = 0.0532 metric = 99.57%\n",
      "Step 94: loss = 0.0519 metric = 99.59%\n",
      "Step 95: loss = 0.0506 metric = 99.64%\n",
      "Step 96: loss = 0.0498 metric = 99.64%\n",
      "Step 97: loss = 0.049 metric = 99.66%\n",
      "Step 98: loss = 0.0483 metric = 99.66%\n",
      "Step 99: loss = 0.0476 metric = 99.68%\n",
      "Step 100: loss = 0.0469 metric = 99.68%\n",
      "Step 101: loss = 0.0463 metric = 99.68%\n",
      "Step 102: loss = 0.0457 metric = 99.70%\n",
      "Step 103: loss = 0.0452 metric = 99.70%\n",
      "Step 104: loss = 0.0448 metric = 99.70%\n",
      "Step 105: loss = 0.0443 metric = 99.70%\n",
      "Step 106: loss = 0.0439 metric = 99.70%\n",
      "Step 107: loss = 0.0434 metric = 99.70%\n",
      "Step 108: loss = 0.043 metric = 99.70%\n",
      "Step 109: loss = 0.0426 metric = 99.70%\n",
      "Step 110: loss = 0.0422 metric = 99.70%\n",
      "Step 111: loss = 0.0418 metric = 99.70%\n",
      "Step 112: loss = 0.0415 metric = 99.70%\n",
      "Step 113: loss = 0.0411 metric = 99.70%\n",
      "Step 114: loss = 0.0407 metric = 99.70%\n",
      "Step 115: loss = 0.0403 metric = 99.70%\n",
      "Step 116: loss = 0.0399 metric = 99.70%\n",
      "Step 117: loss = 0.0395 metric = 99.73%\n",
      "Step 118: loss = 0.0391 metric = 99.73%\n",
      "Step 119: loss = 0.0387 metric = 99.73%\n",
      "Step 120: loss = 0.0384 metric = 99.73%\n",
      "Step 121: loss = 0.038 metric = 99.73%\n",
      "Step 122: loss = 0.0374 metric = 99.75%\n",
      "Step 123: loss = 0.0371 metric = 99.75%\n",
      "Step 124: loss = 0.0368 metric = 99.75%\n",
      "Step 125: loss = 0.0366 metric = 99.75%\n",
      "Step 126: loss = 0.0363 metric = 99.75%\n",
      "Step 127: loss = 0.036 metric = 99.75%\n",
      "Step 128: loss = 0.0357 metric = 99.75%\n",
      "Step 129: loss = 0.0355 metric = 99.77%\n",
      "Step 130: loss = 0.0352 metric = 99.75%\n",
      "Step 131: loss = 0.0349 metric = 99.77%\n",
      "Step 132: loss = 0.0347 metric = 99.75%\n",
      "Step 133: loss = 0.0344 metric = 99.75%\n",
      "Step 134: loss = 0.0342 metric = 99.75%\n",
      "Step 135: loss = 0.0339 metric = 99.75%\n",
      "Step 136: loss = 0.0337 metric = 99.77%\n",
      "Step 137: loss = 0.0334 metric = 99.77%\n",
      "Step 138: loss = 0.0332 metric = 99.77%\n",
      "Step 139: loss = 0.033 metric = 99.77%\n",
      "Step 140: loss = 0.0328 metric = 99.77%\n",
      "Step 141: loss = 0.0325 metric = 99.77%\n",
      "Step 142: loss = 0.0323 metric = 99.77%\n",
      "Step 143: loss = 0.0321 metric = 99.77%\n",
      "Step 144: loss = 0.0319 metric = 99.77%\n",
      "Step 145: loss = 0.0317 metric = 99.77%\n",
      "Step 146: loss = 0.0315 metric = 99.77%\n",
      "Step 147: loss = 0.0313 metric = 99.77%\n",
      "Step 148: loss = 0.0311 metric = 99.77%\n",
      "Step 149: loss = 0.0309 metric = 99.77%\n",
      "Step 150: loss = 0.0307 metric = 99.77%\n",
      "Step 151: loss = 0.0305 metric = 99.77%\n",
      "Step 152: loss = 0.0303 metric = 99.77%\n",
      "Step 153: loss = 0.0302 metric = 99.77%\n",
      "Step 154: loss = 0.03 metric = 99.77%\n",
      "Step 155: loss = 0.0298 metric = 99.77%\n",
      "Step 156: loss = 0.0296 metric = 99.77%\n",
      "Step 157: loss = 0.0294 metric = 99.77%\n",
      "Step 158: loss = 0.0293 metric = 99.77%\n",
      "Step 159: loss = 0.0291 metric = 99.77%\n",
      "Step 160: loss = 0.0289 metric = 99.77%\n",
      "Step 161: loss = 0.0288 metric = 99.77%\n",
      "Step 162: loss = 0.0286 metric = 99.77%\n",
      "Step 163: loss = 0.0285 metric = 99.77%\n",
      "Step 164: loss = 0.0283 metric = 99.77%\n",
      "Step 165: loss = 0.0281 metric = 99.77%\n",
      "Step 166: loss = 0.0279 metric = 99.77%\n",
      "Step 167: loss = 0.0277 metric = 99.77%\n",
      "Step 168: loss = 0.0273 metric = 99.80%\n",
      "Step 169: loss = 0.0271 metric = 99.80%\n",
      "Step 170: loss = 0.027 metric = 99.80%\n",
      "Step 171: loss = 0.0268 metric = 99.80%\n",
      "Step 172: loss = 0.0267 metric = 99.80%\n",
      "Step 173: loss = 0.0265 metric = 99.80%\n",
      "Step 174: loss = 0.0262 metric = 99.80%\n",
      "Step 175: loss = 0.026 metric = 99.80%\n",
      "Step 176: loss = 0.0258 metric = 99.82%\n",
      "Step 177: loss = 0.0256 metric = 99.82%\n",
      "Step 178: loss = 0.0254 metric = 99.82%\n",
      "Step 179: loss = 0.0252 metric = 99.84%\n",
      "Step 180: loss = 0.025 metric = 99.84%\n",
      "Step 181: loss = 0.0249 metric = 99.84%\n",
      "Step 182: loss = 0.0247 metric = 99.84%\n",
      "Step 183: loss = 0.0246 metric = 99.84%\n",
      "Step 184: loss = 0.0245 metric = 99.84%\n",
      "Step 185: loss = 0.0243 metric = 99.84%\n",
      "Step 186: loss = 0.0242 metric = 99.84%\n",
      "Step 187: loss = 0.0241 metric = 99.84%\n",
      "Step 188: loss = 0.024 metric = 99.84%\n",
      "Step 189: loss = 0.0238 metric = 99.84%\n",
      "Step 190: loss = 0.0237 metric = 99.84%\n",
      "Step 191: loss = 0.0236 metric = 99.84%\n",
      "Step 192: loss = 0.0235 metric = 99.84%\n",
      "Step 193: loss = 0.0234 metric = 99.84%\n",
      "Step 194: loss = 0.0233 metric = 99.84%\n",
      "Step 195: loss = 0.0231 metric = 99.84%\n",
      "Step 196: loss = 0.023 metric = 99.84%\n",
      "Step 197: loss = 0.0229 metric = 99.84%\n",
      "Step 198: loss = 0.0228 metric = 99.84%\n",
      "Step 199: loss = 0.0227 metric = 99.84%\n",
      "Step 200: loss = 0.0226 metric = 99.84%\n",
      "Step 201: loss = 0.0225 metric = 99.84%\n",
      "Step 202: loss = 0.0224 metric = 99.84%\n",
      "Step 203: loss = 0.0223 metric = 99.84%\n",
      "Step 204: loss = 0.0222 metric = 99.84%\n",
      "Step 205: loss = 0.0221 metric = 99.84%\n",
      "Step 206: loss = 0.022 metric = 99.84%\n",
      "Step 207: loss = 0.0219 metric = 99.84%\n",
      "Step 208: loss = 0.0218 metric = 99.84%\n",
      "Step 209: loss = 0.0217 metric = 99.84%\n",
      "Step 210: loss = 0.0216 metric = 99.84%\n",
      "Step 211: loss = 0.0215 metric = 99.84%\n",
      "Step 212: loss = 0.0214 metric = 99.84%\n",
      "Step 213: loss = 0.0213 metric = 99.84%\n",
      "Step 214: loss = 0.0212 metric = 99.84%\n",
      "Step 215: loss = 0.0211 metric = 99.84%\n",
      "Step 216: loss = 0.021 metric = 99.84%\n",
      "Step 217: loss = 0.0209 metric = 99.84%\n",
      "Step 218: loss = 0.0208 metric = 99.84%\n",
      "Step 219: loss = 0.0207 metric = 99.84%\n",
      "Step 220: loss = 0.0206 metric = 99.84%\n",
      "Step 221: loss = 0.0206 metric = 99.84%\n",
      "Step 222: loss = 0.0205 metric = 99.84%\n",
      "Step 223: loss = 0.0204 metric = 99.84%\n",
      "Step 224: loss = 0.0203 metric = 99.84%\n",
      "Step 225: loss = 0.0202 metric = 99.84%\n",
      "Step 226: loss = 0.0201 metric = 99.84%\n",
      "Step 227: loss = 0.02 metric = 99.84%\n",
      "Step 228: loss = 0.0199 metric = 99.84%\n",
      "Step 229: loss = 0.0199 metric = 99.84%\n",
      "Step 230: loss = 0.0198 metric = 99.84%\n",
      "Step 231: loss = 0.0197 metric = 99.84%\n",
      "Step 232: loss = 0.0196 metric = 99.84%\n",
      "Step 233: loss = 0.0195 metric = 99.84%\n",
      "Step 234: loss = 0.0195 metric = 99.84%\n",
      "Step 235: loss = 0.0194 metric = 99.84%\n",
      "Step 236: loss = 0.0193 metric = 99.84%\n",
      "Step 237: loss = 0.0192 metric = 99.84%\n",
      "Step 238: loss = 0.0191 metric = 99.84%\n",
      "Step 239: loss = 0.0191 metric = 99.84%\n",
      "Step 240: loss = 0.019 metric = 99.84%\n",
      "Step 241: loss = 0.0189 metric = 99.84%\n",
      "Step 242: loss = 0.0189 metric = 99.84%\n",
      "Step 243: loss = 0.0188 metric = 99.84%\n",
      "Step 244: loss = 0.0187 metric = 99.84%\n",
      "Step 245: loss = 0.0186 metric = 99.84%\n",
      "Step 246: loss = 0.0186 metric = 99.84%\n",
      "Step 247: loss = 0.0185 metric = 99.84%\n",
      "Step 248: loss = 0.0184 metric = 99.84%\n",
      "Step 249: loss = 0.0183 metric = 99.84%\n",
      "Step 250: loss = 0.0183 metric = 99.84%\n",
      "Step 251: loss = 0.0182 metric = 99.84%\n",
      "Step 252: loss = 0.0181 metric = 99.84%\n",
      "Step 253: loss = 0.0181 metric = 99.84%\n",
      "Step 254: loss = 0.018 metric = 99.84%\n",
      "Step 255: loss = 0.0179 metric = 99.84%\n",
      "Step 256: loss = 0.0179 metric = 99.84%\n",
      "Step 257: loss = 0.0178 metric = 99.84%\n",
      "Step 258: loss = 0.0177 metric = 99.84%\n",
      "Step 259: loss = 0.0177 metric = 99.84%\n",
      "Step 260: loss = 0.0176 metric = 99.84%\n",
      "Step 261: loss = 0.0176 metric = 99.84%\n",
      "Step 262: loss = 0.0175 metric = 99.84%\n",
      "Step 263: loss = 0.0174 metric = 99.84%\n",
      "Step 264: loss = 0.0174 metric = 99.84%\n",
      "Step 265: loss = 0.0173 metric = 99.84%\n",
      "Step 266: loss = 0.0172 metric = 99.84%\n",
      "Step 267: loss = 0.0172 metric = 99.84%\n",
      "Step 268: loss = 0.0171 metric = 99.84%\n",
      "Step 269: loss = 0.0171 metric = 99.84%\n",
      "Step 270: loss = 0.017 metric = 99.84%\n",
      "Step 271: loss = 0.0169 metric = 99.84%\n",
      "Step 272: loss = 0.0169 metric = 99.84%\n",
      "Step 273: loss = 0.0168 metric = 99.84%\n",
      "Step 274: loss = 0.0168 metric = 99.84%\n",
      "Step 275: loss = 0.0167 metric = 99.84%\n",
      "Step 276: loss = 0.0167 metric = 99.84%\n",
      "Step 277: loss = 0.0166 metric = 99.84%\n",
      "Step 278: loss = 0.0165 metric = 99.84%\n",
      "Step 279: loss = 0.0165 metric = 99.84%\n",
      "early stoping\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(len(labels), tokenizer, 4, 16, 8)\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0E-3)\n",
    "train = Batchifyer(df_train, tokenizer, n_batches=1, batch_size=None)\n",
    "val = Batchifyer(df_val, tokenizer, n_batches=1, batch_size=None)\n",
    "train_loop(model, optimizer, train, val, n_steps=1000, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 99.772%\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_to_tensor(df_test, tokenizer)\n",
    "y_pred = model.predict(X)\n",
    "acc = accuracy(y_pred, Y.to(y_pred.device))\n",
    "print(f\"accuracy {acc:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2017a09ab1f54c7a6a1af190715fd60264df4a93389a277e03c18d947a6e489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
